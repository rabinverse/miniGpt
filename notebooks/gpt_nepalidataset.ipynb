{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e06db4",
   "metadata": {},
   "source": [
    "<img src=\"../dataset_research_paper_docs/transformer_archi.png\" alt=\"transformer\" width=\"600\" style=\"display:block; margin:auto;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6168dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8438d279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 51, 49, 50, 36]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[45, 51, 49, 50, 36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9347f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[45, 51, 49, 50, 36]],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d5630a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9715644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/2yb8tvwd13q0h205019lpz540000gn/T/ipykernel_32237/4163618824.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b=torch.tensor(torch.zeros((1,1),dtype=torch.long))\n"
     ]
    }
   ],
   "source": [
    "b=torch.tensor(torch.zeros((1,1),dtype=torch.long))\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86778a65",
   "metadata": {},
   "source": [
    "# Char level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846129f7",
   "metadata": {},
   "source": [
    "based on the dataset of sheskphreare's works , the goal is to build a character level language model using transformer architecture.\n",
    "\n",
    "- given a sequence of characters , the model will predict the next character in the sequence.\n",
    "- `Goal`: imitate the style of sheskphreare's writing.\n",
    "\n",
    "- character level encoding\n",
    "\n",
    "## components used\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa00688",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08ec56",
   "metadata": {},
   "source": [
    "# Trained on Narayan Gopal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095db2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1329422b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba565484",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset_research_paper_docs/nepalidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a38df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6081"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d2ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "चिनारी हाम्रो धेरै पुरानो:\n",
      "चिनारी हाम्रो, धेरै पुरानो रहेझै लाग्दछ।\n",
      "नदेखे पनि भेट सधै-सधै, भएझै लाग्दछ।\n",
      "\n",
      "शीतल दिन्छौ, पवन भएर, मिर्मिरे याममा\n",
      "मायालु न्यानो स्पर्ष दिन्छौ, किरणको रूपमा।\n",
      "झनझनै बस्छ, संगीत तिम्रो, हरेक तन्तुमा।\n",
      "आशाको दियो आफै नै बल्छ, अँध्यारो कुनामा।\n",
      "\n",
      "हजार खोजे, त्यो रुप हेर्न, पाउदै पाईन।\n",
      "मायाले छुने, हाथ छाम्न खोजे, समाउनै सकिन।\n",
      "भनिदेउ आज, को हौ तिमी, मैले त जानिन।\n",
      "विरंगी रुप, फेरेर आउने, तिमीलाई चिनिन।\n",
      "\n",
      "चुमेर पाना भरि :\n",
      "चुमेर पाना भरि यति म लेखि दिन्छु\n",
      "प्रितिका बाचाहरु जाने जति\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257b9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "{'थ', 'ा', 'ं', 'ह', 'ँ', '(', 'व', 'स', 'य', 'ऊ', 'ृ', 'ी', 'ढ', '\\u2005', 'ड़', '.', ' ', 'घ', 'फ', 'भ', '-', 'ौ', 'ु', '\\n', 'ठ', 'ध', ')', 'प', 'च', 'ए', 'े', 'त', 'ब', 'अ', 'ि', 'ग', 'ष', ':', 'ै', 'छ', 'श', '।', 'ो', 'उ', 'न', '्', 'ल', 'ड', 'ट', 'द', ',', 'झ', 'क', 'ङ', 'म', 'ू', 'ज', 'ण', 'ई', 'इ', 'र', 'आ', 'ख', '\\xa0'}\n"
     ]
    }
   ],
   "source": [
    "chars = set(text)\n",
    "print(len(chars))\n",
    "print(chars, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65def49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['थ', 'ा', 'ं', 'ह', 'ँ', '(', 'व', 'स', 'य', 'ऊ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145637e9",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e7ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (),-.: ँंअआइईउऊएकखगघङचछजझटठडढणतथदधनपफबभमयरलवशषसहािीुूृेैोौ्ड़। \n",
      "64\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10679d",
   "metadata": {},
   "source": [
    "## `create` a mapping table for string to integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a83b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strtoint = {ch: i for i, ch in enumerate(chars)}\n",
    "inttostr = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode_txt = lambda s: [strtoint[c] for c in s]\n",
    "# returns list of integer for input string given\n",
    "\n",
    "decode_txt = lambda l: \"\".join(inttostr[i] for i in l)\n",
    "# returns string from given integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64b93f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0),\n",
       " (' ', 1),\n",
       " ('(', 2),\n",
       " (')', 3),\n",
       " (',', 4),\n",
       " ('-', 5),\n",
       " ('.', 6),\n",
       " (':', 7),\n",
       " ('\\xa0', 8),\n",
       " ('ँ', 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[:10]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6280d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ू', 54),\n",
       " ('ृ', 55),\n",
       " ('े', 56),\n",
       " ('ै', 57),\n",
       " ('ो', 58),\n",
       " ('ौ', 59),\n",
       " ('्', 60),\n",
       " ('ड़', 61),\n",
       " ('।', 62),\n",
       " ('\\u2005', 63)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[-10:]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b2be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(54, 'ू'),\n",
       " (55, 'ृ'),\n",
       " (56, 'े'),\n",
       " (57, 'ै'),\n",
       " (58, 'ो'),\n",
       " (59, 'ौ'),\n",
       " (60, '्'),\n",
       " (61, 'ड़'),\n",
       " (62, '।'),\n",
       " (63, '\\u2005')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inttostr.items())[-10:]  # lookuptable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eaaaf8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30417",
   "metadata": {},
   "source": [
    "Character level token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7bdd908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 11, 9, 35, 60, 42, 50, 43, 58, 1, 18, 53, 36, 50, 41, 50, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_txt(\" अँध्यारो कुनामा \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f452d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' इअप।लीव् गृफीरी '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([1, 13, 11, 37, 62, 44, 52, 45, 60, 1, 20, 55, 38, 52, 43, 52, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc598895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 41, 48, 60, 32, 56, 1, 48, 50, 33, 52, 49, 43, 54, 4, 1, 18, 56, 1, 24, 1, 49, 50, 44, 19, 39, 43]\n",
      "नमस्ते साथीहरू, के छ हालखबर\n"
     ]
    }
   ],
   "source": [
    "print(encode_txt(\"नमस्ते साथीहरू, के छ हालखबर\"))\n",
    "\n",
    "enc_text = encode_txt(\"नमस्ते साथीहरू, के छ हालखबर\")\n",
    "\n",
    "print(decode_txt(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7385e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 51, 49, 50, 36]\n",
      "विहान\n"
     ]
    }
   ],
   "source": [
    "print(encode_txt(\"विहान\"))\n",
    "\n",
    "enc_text = encode_txt(\"विहान\")\n",
    "\n",
    "print(decode_txt(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590ad12",
   "metadata": {},
   "source": [
    "# Google uses [sentencepiece](https://github.com/google/sentencepiece) for tokenization.\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e553b",
   "metadata": {},
   "source": [
    "# OpenAI uses Byte Pair Encoding [BPE](https://github.com/openai/tiktoken) for tokenization.\n",
    "\n",
    "BPE is a simple form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. In the context of tokenization, BPE is used to create a vocabulary of subword units that can efficiently represent text data. The algorithm starts with a base vocabulary of individual characters and then merges the most frequent pairs of characters or subwords to form new tokens. This process continues until a predefined vocabulary size is reached. BPE is particularly effective for handling out-of-vocabulary words and capturing common patterns in text, making it a popular choice for tokenization in natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0635cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f1451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d8576c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8a4be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11976, 101, 11976, 106, 11976, 116, 24231, 235, 11976, 97, 24231, 229, 28225, 116, 48077, 11976, 98, 24231, 222, 11976, 117, 11976, 108, 24231, 224, 11, 28225, 243, 24231, 229, 28225, 249, 28225, 117, 48077, 11976, 110, 11976, 244, 11976, 105, 11976, 108, 30]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'नमस्ते साथीहरू, के छ हालखबर?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(enc.encode(\"नमस्ते साथीहरू, के छ हालखबर?\"))\n",
    "that = enc.encode(\"नमस्ते साथीहरू, के छ हालखबर?\")\n",
    "enc.decode(that)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406dd94",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46892bc4",
   "metadata": {},
   "source": [
    "`Encode` the whole narayan gopal text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00349e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'चिनारी हाम्रो, धेरै पुरानो रहेझै लाग्दछ।\\nनदेखे पनि'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af37bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5997]) torch.int64\n",
      "tensor([25, 53, 38, 52, 45, 54,  1, 51, 52, 43, 62, 45, 60,  4,  1, 37, 58, 45,\n",
      "        59,  1, 39, 55, 45, 52, 38, 60,  1, 45, 51, 58, 28, 59,  1, 46, 52, 22,\n",
      "        62, 36, 26, 64,  0, 38, 36, 58, 21, 58,  1, 39, 38, 53,  1, 42, 58, 29,\n",
      "         1, 50, 37, 59,  5, 50, 37, 59,  4,  1, 42, 19, 28, 59,  1, 46, 52, 22,\n",
      "        62, 36, 26, 64,  0, 48, 54, 34, 46,  1, 36, 53, 38, 62, 26, 61,  4,  1,\n",
      "        39, 47, 38,  1, 42, 19, 45,  4,  1, 43, 53, 45, 62, 43, 53, 45, 58,  1,\n",
      "        44, 52, 43, 43, 52,  0, 43, 52, 44, 52, 46, 55,  1, 38, 62, 44, 52, 38,\n",
      "        60,  1, 50, 62, 39, 45, 62, 49,  1, 36, 53, 38, 62, 26, 61,  4,  1, 20,\n",
      "        53, 45, 33, 20, 60,  1, 45, 56, 39, 43, 52, 64,  0, 28, 38, 28, 38, 59,\n",
      "         1, 41, 50, 62, 26,  4,  1, 50, 12, 22, 54, 34,  1, 34, 53, 43, 62, 45,\n",
      "        60,  4,  1, 51, 45, 58, 20,  1, 34, 38, 62, 34, 55, 43, 52, 64,  0, 14,\n",
      "        48, 52, 20, 60,  1, 36, 53, 44, 60,  1, 14, 40, 59,  1, 38, 59,  1, 41,\n",
      "        46, 62, 26,  4,  1, 13, 11, 37, 62, 44, 52, 45, 60,  1, 20, 55, 38, 52,\n",
      "        43, 52, 64,  0, 51, 27, 52, 45,  1, 21, 60, 27, 58,  4,  1, 34, 62, 44,\n",
      "        60,  1, 45, 55, 39,  1, 51, 58, 45, 62, 38,  4,  1, 39, 52, 17, 36, 59,\n",
      "         1, 39, 52, 16, 38, 64,  0, 43, 52, 44, 52, 46, 58,  1, 26, 55, 38, 58,\n",
      "         4,  1, 51, 52, 35,  1, 26, 52, 43, 62, 38,  1, 21, 60, 27, 58,  4,  1,\n",
      "        50, 43, 52, 17, 38, 59,  1, 50, 20, 53, 38, 64,  0, 42, 38, 53, 36, 58,\n",
      "        17,  1, 14, 27,  4,  1, 20, 60,  1, 51, 61,  1, 34, 53, 43, 54,  4,  1,\n",
      "        43, 59, 46, 58,  1, 34,  1, 27, 52, 38, 53, 38, 64,  0, 47, 53, 45, 12,\n",
      "        22, 54,  1, 45, 55, 39,  4,  1, 40, 58, 45, 58, 45,  1, 14, 17, 38, 58,\n",
      "         4,  1, 34, 53, 43, 54, 46, 52, 16,  1, 25, 53, 38, 53, 38, 64,  0, 25,\n",
      "        55, 43, 58, 45,  1, 39, 52, 38, 52,  1, 42, 45, 53,  1,  0,  8,  9,  0,\n",
      "        25, 55, 43, 58, 45,  1, 39, 52, 38, 52,  1, 42, 45, 53,  1, 44, 34, 53,\n",
      "         1, 43,  1, 46, 58, 21, 53,  1, 36, 53, 38, 62, 26, 55,  0, 39, 62, 45,\n",
      "        53, 34, 53, 20, 52,  1, 41, 52, 25, 52, 51, 45, 55,  1, 27, 52, 38, 58,\n",
      "         1, 27, 34, 53,  1, 46, 58, 21, 53,  1, 36, 53, 38, 62, 26, 55,  0, 45,\n",
      "        52, 34, 42, 45, 54,  1, 42, 45, 53,  1, 19, 17, 29, 59])\n"
     ]
    }
   ],
   "source": [
    "# encode whole text\n",
    "data = torch.tensor(encode_txt(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "# print first 500 character encoding\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81db651",
   "metadata": {},
   "source": [
    "# `split` the data to train test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db1149ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5997])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94fdfda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5397\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "print(n)\n",
    "\n",
    "\n",
    "# first 90% in the train and rest 10% in the val\n",
    "\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e67a9",
   "metadata": {},
   "source": [
    "while training we dont give the model the full sequence rather we give part of the sequence and do it in batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12c6ca",
   "metadata": {},
   "source": [
    "block size or context length : how many tokens the model can see at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1f32549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 53, 38, 52, 45, 54,  1, 51, 52])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d6cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given -> tensor([25, 53, 38, 52, 45, 54,  1, 51]) predict -> tensor(52) total -> tensor([25, 53, 38, 52, 45, 54,  1, 51, 52])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"given ->\",\n",
    "    train_data[:block_size],\n",
    "    \"predict ->\",\n",
    "    train_data[block_size],\n",
    "    \"total ->\",\n",
    "    train_data[: block_size + 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47de7c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([25]) o/p --->  tensor(53)\n",
      "when input is  tensor([25, 53]) o/p --->  tensor(38)\n",
      "when input is  tensor([25, 53, 38]) o/p --->  tensor(52)\n",
      "when input is  tensor([25, 53, 38, 52]) o/p --->  tensor(45)\n",
      "when input is  tensor([25, 53, 38, 52, 45]) o/p --->  tensor(54)\n",
      "when input is  tensor([25, 53, 38, 52, 45, 54]) o/p --->  tensor(1)\n",
      "when input is  tensor([25, 53, 38, 52, 45, 54,  1]) o/p --->  tensor(51)\n",
      "when input is  tensor([25, 53, 38, 52, 45, 54,  1, 51]) o/p --->  tensor(52)\n"
     ]
    }
   ],
   "source": [
    "# x is the input to the transformer --first block size characters\n",
    "# y is offset by 1 to x ----- next block size character. - y is the target for each position to the input\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(\"when input is \", context, \"o/p ---> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26999319",
   "metadata": {},
   "source": [
    "there is a new dimension batch dimension\n",
    "while training we dont give the model the full sequence rather we give part of the sequence and do it in batches.\n",
    "\n",
    "batches of sequences of block size length are fed for efficiency to process in parallel\n",
    "\n",
    "batch of sequence of block size length are stacked in tensor and fed to process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "047546ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # length of the input sequence\n",
    "batch_size = 4  # no of input sequence to process in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65cee5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5989"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) - block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5b45716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5389"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) - block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c5f818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 414, 4726,  281, 4005])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# four independent rows\n",
    "\n",
    "\n",
    "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "331924ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(414)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(25)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ix[0])\n",
    "data[ix[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efc030",
   "metadata": {},
   "source": [
    "in a batch,completely independent sequences are selected randomly of block size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b19b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[25, 55, 43, 58, 45,  1, 39, 52],\n",
      "        [11, 31, 62, 38, 55,  1, 39, 45],\n",
      "        [46, 58,  1, 26, 55, 38, 58,  4],\n",
      "        [45,  0, 44, 51, 52, 11,  1, 32]])\n",
      "tensor([[55, 43, 58, 45,  1, 39, 52, 38],\n",
      "        [31, 62, 38, 55,  1, 39, 45, 62],\n",
      "        [58,  1, 26, 55, 38, 58,  4,  1],\n",
      "        [ 0, 44, 51, 52, 11,  1, 32, 55]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89103742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdac65",
   "metadata": {},
   "source": [
    "- when generating block size of context length(block size continuous adjacent data) ,subtract len(data)-block size to avoid out of index\n",
    "- ie block size needed is 8 and if data length is 100 and block size is 8 ,the max starting point can be 92 to get 92 to 100 (8 length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e727877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8])\n",
      "tensor([[62, 22,  1, 27, 54, 38, 62, 36],\n",
      "        [60,  1, 45, 55, 39,  1, 51, 58],\n",
      "        [52, 19,  1, 27, 50, 62, 34, 60],\n",
      "        [46,  1, 26, 38, 62,  1, 14, 11]])\n",
      "----\n",
      " \n",
      "targets:  torch.Size([4, 8])\n",
      "tensor([[22,  1, 27, 54, 38, 62, 36, 22],\n",
      "        [ 1, 45, 55, 39,  1, 51, 58, 45],\n",
      "        [19,  1, 27, 50, 62, 34, 60,  1],\n",
      "        [ 1, 26, 38, 62,  1, 14, 11, 21]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8  # length of the input sequence\n",
    "batch_size = 4  # no of input sequence to process in parallel\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs: \", xb.shape)\n",
    "print(xb)\n",
    "print(\"----\\n \")\n",
    "print(\"targets: \", yb.shape)\n",
    "print(yb)\n",
    "\n",
    "# xb is the input to the transformer\n",
    "\n",
    "# for 54 target is 43,for 43 target is 39 for single token target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab6bbecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8\n"
     ]
    }
   ],
   "source": [
    "print(batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a17f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  [62] output -->  tensor(22)\n",
      "when input is  [62, 22] output -->  tensor(1)\n",
      "when input is  [62, 22, 1] output -->  tensor(27)\n",
      "when input is  [62, 22, 1, 27] output -->  tensor(54)\n",
      "when input is  [62, 22, 1, 27, 54] output -->  tensor(38)\n",
      "when input is  [62, 22, 1, 27, 54, 38] output -->  tensor(62)\n",
      "when input is  [62, 22, 1, 27, 54, 38, 62] output -->  tensor(36)\n",
      "when input is  [62, 22, 1, 27, 54, 38, 62, 36] output -->  tensor(22)\n",
      "when input is  [60] output -->  tensor(1)\n",
      "when input is  [60, 1] output -->  tensor(45)\n",
      "when input is  [60, 1, 45] output -->  tensor(55)\n",
      "when input is  [60, 1, 45, 55] output -->  tensor(39)\n",
      "when input is  [60, 1, 45, 55, 39] output -->  tensor(1)\n",
      "when input is  [60, 1, 45, 55, 39, 1] output -->  tensor(51)\n",
      "when input is  [60, 1, 45, 55, 39, 1, 51] output -->  tensor(58)\n",
      "when input is  [60, 1, 45, 55, 39, 1, 51, 58] output -->  tensor(45)\n",
      "when input is  [52] output -->  tensor(19)\n",
      "when input is  [52, 19] output -->  tensor(1)\n",
      "when input is  [52, 19, 1] output -->  tensor(27)\n",
      "when input is  [52, 19, 1, 27] output -->  tensor(50)\n",
      "when input is  [52, 19, 1, 27, 50] output -->  tensor(62)\n",
      "when input is  [52, 19, 1, 27, 50, 62] output -->  tensor(34)\n",
      "when input is  [52, 19, 1, 27, 50, 62, 34] output -->  tensor(60)\n",
      "when input is  [52, 19, 1, 27, 50, 62, 34, 60] output -->  tensor(1)\n",
      "when input is  [46] output -->  tensor(1)\n",
      "when input is  [46, 1] output -->  tensor(26)\n",
      "when input is  [46, 1, 26] output -->  tensor(38)\n",
      "when input is  [46, 1, 26, 38] output -->  tensor(62)\n",
      "when input is  [46, 1, 26, 38, 62] output -->  tensor(1)\n",
      "when input is  [46, 1, 26, 38, 62, 1] output -->  tensor(14)\n",
      "when input is  [46, 1, 26, 38, 62, 1, 14] output -->  tensor(11)\n",
      "when input is  [46, 1, 26, 38, 62, 1, 14, 11] output -->  tensor(21)\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        context = xb[i, : j + 1]\n",
    "        target = yb[i, j]\n",
    "        print(\"when input is \", context.tolist(), \"output --> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b1c2",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b639ae",
   "metadata": {},
   "source": [
    "## start feeding to NN\n",
    "\n",
    "- Bigram model\n",
    "  - simple model for language modeling that predicts the next token based on the current token using a lookup table.\n",
    "  - each token in the vocabulary has a corresponding embedding vector in the lookup table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8e1d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of size (vocab_size, vocab_size) where each row corresponds to a token in the vocabulary and contains the logits for predicting the next token.\n",
    "\n",
    "# here each token is made to (65\\*65)\n",
    "\n",
    "# Embedding =  matrix of shape (num_embeddings, embedding_dim)\n",
    "\n",
    "# when\n",
    "# logits = self.token_embedding_table(idx)\n",
    "# internally\n",
    "# logits[b, t] = W[idx[b, t]]\n",
    "\n",
    "# The embedding table is formed by initializing a (vocab_size × vocab_size) matrix with random values and then gradually shaping each row through gradient descent so that it learns the logits for predicting the next token given the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e1996f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 66])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        # here logits/embedding value\n",
    "        return logits\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)\n",
    "\n",
    "# idx or xb =(4,8)\n",
    "# returned logits= (4,8,65)(4batch of 8dim with 65vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44dd6bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76f80b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 31, 62, 38, 55,  1, 39, 45])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ff2ed",
   "metadata": {},
   "source": [
    "#returned logits= (4,8,65)(4batch of 65dim vector for each of the 8 tokens in the sequence)\n",
    "\n",
    "- Each integer in the 8-length vector becomes a 65-length vector\n",
    "- for x[4,8] 4 vec of 8dimlength each logits returns as (4,8,65) ,4batch of 65dim vector for each of the 8 tokens in the sequence\n",
    "- [ 0, 32, 46, 53, 59, 1, 40, 43] each integer in 8-length vector becomes a 65-length vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2afe94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 66])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cd48a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.4622e-01,  2.7878e-01,  7.2800e-01, -8.1086e-01,  2.4097e-01,\n",
       "          -1.1390e-01,  7.5586e-02, -1.6272e-02,  3.8805e-01,  5.8376e-01,\n",
       "          -6.1043e-01, -1.4693e-01, -5.6213e-01, -7.8606e-01,  1.0935e+00,\n",
       "          -6.7555e-02,  4.0387e-01, -1.9338e+00,  5.6823e-01, -7.4776e-01,\n",
       "          -6.9247e-01,  1.8697e+00, -3.1178e-01, -2.3696e-02,  1.3093e+00,\n",
       "           3.1762e-01, -4.0408e-02, -1.3968e+00, -7.7044e-01,  1.2933e+00,\n",
       "           2.9970e-01, -7.4225e-01,  7.7630e-01, -9.3699e-01,  1.3495e-01,\n",
       "          -3.2095e-01, -8.0112e-01,  8.1163e-02, -1.1384e+00, -4.4760e-01,\n",
       "           2.2702e-01,  1.5843e+00,  1.1594e+00,  9.2629e-01,  3.1651e-01,\n",
       "          -5.5963e-01,  1.1860e+00,  7.1131e-02, -1.8267e+00,  1.9844e+00,\n",
       "           7.0427e-01,  2.0181e-01,  1.4936e-02,  1.3187e+00,  7.8984e-01,\n",
       "           4.5662e-01, -2.8259e+00, -1.3648e+00, -7.7053e-01, -1.9718e+00,\n",
       "           3.7323e-01,  9.7256e-01,  7.6028e-01, -7.4973e-01,  4.7539e-02,\n",
       "           1.4509e+00],\n",
       "         [ 8.6961e-01,  7.9524e-01, -4.2110e-01,  1.6545e+00, -7.5400e-02,\n",
       "           8.1369e-01,  1.5688e-01, -8.8361e-01, -1.3572e+00,  2.5191e-01,\n",
       "          -7.1810e-01,  1.2180e+00, -6.7451e-01,  1.0277e+00,  1.2858e+00,\n",
       "          -5.7101e-01, -2.6378e-01,  1.1981e+00,  7.8190e-01, -1.3479e-01,\n",
       "           1.0729e+00, -1.1304e+00, -1.2815e+00,  1.2645e+00,  7.2935e-01,\n",
       "           1.2863e+00,  1.0577e+00, -5.4153e-01,  4.6853e-01,  1.2207e+00,\n",
       "           8.1351e-02, -8.2789e-01,  8.5134e-01,  1.2136e-01,  1.2588e+00,\n",
       "           6.4686e-01,  1.9593e-01,  4.5211e-01,  1.6208e-01, -1.8787e-01,\n",
       "          -5.5469e-01,  2.0841e-01,  4.0004e-01, -1.5766e+00,  6.6795e-01,\n",
       "          -1.0362e+00,  1.6007e+00, -7.1171e-01, -4.6747e-01, -1.1337e+00,\n",
       "          -1.2783e+00, -4.3894e-01, -8.6417e-01,  7.7469e-01, -2.7729e-01,\n",
       "          -2.6471e-01,  1.2861e+00,  1.0044e+00,  2.3729e-02,  5.7260e-01,\n",
       "          -6.5357e-01, -6.6449e-01,  2.7637e-01,  1.9566e+00,  1.3431e+00,\n",
       "           1.6737e-01],\n",
       "         [-5.1406e-02, -6.4559e-02, -4.9701e-01,  4.6576e-01, -2.5726e-01,\n",
       "          -1.0673e+00,  2.0089e+00, -5.3698e-01,  2.2280e-01,  6.9705e-01,\n",
       "          -1.4267e+00,  9.0594e-01,  1.4459e-01,  2.2800e-01,  2.4900e+00,\n",
       "          -1.2237e+00,  1.0107e+00,  5.5600e-01, -1.5935e+00, -1.2706e+00,\n",
       "           6.9033e-01, -1.9614e-01,  3.4491e-01, -3.4189e-01,  4.7587e-01,\n",
       "          -7.6634e-01, -4.1896e-01, -4.3699e-01, -1.0012e+00, -4.0943e-01,\n",
       "          -1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,  3.1549e-02,\n",
       "          -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01, -1.3343e-01,\n",
       "           2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00, -6.4046e-01,\n",
       "          -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01, -1.8846e+00,\n",
       "           1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,  1.7510e+00,\n",
       "           2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,  3.1845e-01,\n",
       "          -5.4959e-01, -1.4649e+00, -2.0555e+00,  1.8275e+00,  1.3035e+00,\n",
       "          -4.5013e-01],\n",
       "         [ 1.7849e-01, -1.0140e+00, -1.9128e-01,  2.4321e-01, -1.2693e+00,\n",
       "           9.7232e-01, -2.3445e-01,  2.8286e-01,  4.2698e-01,  6.4930e-01,\n",
       "          -3.0116e-01, -4.9007e-01, -1.3679e+00,  2.2490e+00,  5.6824e-01,\n",
       "           1.5880e+00, -7.3346e-01, -1.6787e+00, -3.3569e-02, -1.5213e+00,\n",
       "           3.8861e-01,  1.0050e+00, -1.2381e+00,  1.3319e+00,  1.5385e-01,\n",
       "           6.3762e-01, -7.4279e-01,  1.6414e+00, -2.6803e-01, -4.5431e-01,\n",
       "           7.1758e-01,  3.6350e-01, -1.1256e+00,  9.4219e-01,  8.8378e-01,\n",
       "          -1.9908e+00,  8.5744e-01, -2.1603e+00,  3.1397e-01,  2.2434e+00,\n",
       "           1.6029e+00, -7.2497e-01,  2.9983e-01, -8.8427e-01,  1.5462e+00,\n",
       "          -7.6456e-01, -2.4664e-01, -6.2308e-01, -4.7668e-02, -2.0922e+00,\n",
       "           2.8453e-02,  1.2881e-01, -7.6382e-01,  9.9616e-02,  2.8292e-01,\n",
       "          -1.2844e+00, -2.7384e-02,  1.1135e-01,  9.4199e-01,  4.3506e-02,\n",
       "           1.2219e+00, -7.7057e-01,  6.8530e-01, -6.4845e-01,  6.4989e-01,\n",
       "          -7.2935e-02],\n",
       "         [ 3.5597e-01, -1.6589e+00,  6.5434e-01, -1.3299e+00,  1.1929e+00,\n",
       "           4.8549e-01, -5.7211e-01,  1.0813e+00,  2.3671e+00, -7.7751e-01,\n",
       "          -2.5861e-01, -1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01,\n",
       "          -2.0244e+00, -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01,\n",
       "          -1.9079e+00, -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01,\n",
       "          -8.9111e-01, -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00,\n",
       "          -1.5821e-01,  1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01,\n",
       "          -1.3412e+00,  3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,\n",
       "           1.3495e-01,  7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,\n",
       "           1.1458e+00, -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02,\n",
       "          -4.9708e-01,  6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,\n",
       "           1.6021e+00, -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00,\n",
       "          -7.1166e-01, -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,\n",
       "           9.1433e-01],\n",
       "         [-6.4792e-01,  8.2067e-01,  4.3697e-01,  9.4258e-02,  8.6093e-01,\n",
       "          -2.4744e-01, -2.3996e+00,  3.8961e-01, -4.0668e-01, -9.9006e-01,\n",
       "          -7.0569e-01, -1.5055e+00, -6.7100e-01, -9.7896e-01, -9.2280e-01,\n",
       "          -5.8793e-01,  3.1401e-01, -3.0934e-01,  4.4559e-03, -8.2324e-01,\n",
       "          -4.6221e-01, -1.3261e+00,  1.1315e+00, -5.4761e-01,  1.1353e+00,\n",
       "           7.5425e-01, -1.1444e+00,  1.1513e+00,  1.0539e+00,  3.4105e+00,\n",
       "          -9.6206e-01, -1.1720e+00,  5.9532e-01, -4.0978e-01,  1.4256e+00,\n",
       "          -1.2171e+00, -1.6845e+00,  5.3848e-01,  1.8967e+00, -2.7450e-01,\n",
       "           2.7868e-01, -6.4734e-01, -2.6276e+00, -1.3731e+00, -1.2415e+00,\n",
       "           7.0759e-01, -4.9464e-01,  1.1809e+00,  5.4237e-01, -8.5781e-01,\n",
       "           5.1982e-01,  1.5089e-01, -3.9927e-02,  1.0038e+00, -1.1435e+00,\n",
       "           1.8040e+00, -2.9009e-02, -8.1313e-01,  9.0933e-01, -1.1375e+00,\n",
       "           5.1402e-01, -4.8947e-01, -8.0550e-02,  9.1511e-01, -5.4810e-01,\n",
       "           1.1071e+00],\n",
       "         [ 5.4622e-01,  2.7878e-01,  7.2800e-01, -8.1086e-01,  2.4097e-01,\n",
       "          -1.1390e-01,  7.5586e-02, -1.6272e-02,  3.8805e-01,  5.8376e-01,\n",
       "          -6.1043e-01, -1.4693e-01, -5.6213e-01, -7.8606e-01,  1.0935e+00,\n",
       "          -6.7555e-02,  4.0387e-01, -1.9338e+00,  5.6823e-01, -7.4776e-01,\n",
       "          -6.9247e-01,  1.8697e+00, -3.1178e-01, -2.3696e-02,  1.3093e+00,\n",
       "           3.1762e-01, -4.0408e-02, -1.3968e+00, -7.7044e-01,  1.2933e+00,\n",
       "           2.9970e-01, -7.4225e-01,  7.7630e-01, -9.3699e-01,  1.3495e-01,\n",
       "          -3.2095e-01, -8.0112e-01,  8.1163e-02, -1.1384e+00, -4.4760e-01,\n",
       "           2.2702e-01,  1.5843e+00,  1.1594e+00,  9.2629e-01,  3.1651e-01,\n",
       "          -5.5963e-01,  1.1860e+00,  7.1131e-02, -1.8267e+00,  1.9844e+00,\n",
       "           7.0427e-01,  2.0181e-01,  1.4936e-02,  1.3187e+00,  7.8984e-01,\n",
       "           4.5662e-01, -2.8259e+00, -1.3648e+00, -7.7053e-01, -1.9718e+00,\n",
       "           3.7323e-01,  9.7256e-01,  7.6028e-01, -7.4973e-01,  4.7539e-02,\n",
       "           1.4509e+00],\n",
       "         [ 1.4769e+00,  6.8216e-01, -8.8222e-01,  7.7618e-01, -6.2467e-02,\n",
       "           8.4428e-01, -1.5745e-02,  1.7251e+00, -1.9974e+00,  4.0745e-01,\n",
       "          -1.5205e+00,  8.7902e-01, -6.0891e-02, -2.0592e-01,  1.3214e+00,\n",
       "           6.6606e-01,  2.9364e-01, -6.1167e-01, -8.8674e-01, -1.3853e-01,\n",
       "           5.8954e-01, -2.2584e-01, -9.9184e-01, -1.0729e+00, -2.4981e-01,\n",
       "          -8.8683e-01,  1.5427e+00,  1.0735e+00, -4.7932e-02, -9.8621e-01,\n",
       "          -3.7870e-02,  1.5067e+00, -3.3953e-01, -5.9732e-01,  3.7160e-01,\n",
       "           1.0398e+00,  9.3845e-02,  4.5692e-01,  6.8518e-01,  2.7331e-01,\n",
       "          -1.1856e+00, -6.8176e-01,  7.8288e-02, -7.3696e-02,  1.9823e+00,\n",
       "           1.0637e+00, -4.0367e-01, -1.1271e-01,  1.5287e+00,  1.7637e-03,\n",
       "           1.4420e+00, -1.2854e+00,  9.8712e-01,  4.5210e-02,  6.2373e-01,\n",
       "           4.5656e-01, -1.1861e+00, -4.5271e-01,  1.4427e+00, -1.6307e+00,\n",
       "          -2.1066e-01,  6.1826e-02,  1.1956e+00, -6.0344e-01,  1.7504e+00,\n",
       "           2.1311e-02]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d9238bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 66]) \n",
      " loss=  tensor(4.6947, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)ie(4,8,65)\n",
    "        # here logits/embedding value\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "\n",
    "        targets = targets.view(B * T)  # (32)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "# idx or xb =(4,8)\n",
    "# returned logits= (4*8,65) stretched vec\n",
    "# losscalculation\n",
    "# The 65-logit vector represents a distribution over choices.\n",
    "# The target integer selects the correct choice, and the loss measures how much probability the model assigned to that choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264e04f",
   "metadata": {},
   "source": [
    "[cross-entropy loss docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ba226e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5462,  0.2788,  0.7280,  ..., -0.7497,  0.0475,  1.4509],\n",
      "        [ 0.8696,  0.7952, -0.4211,  ...,  1.9566,  1.3431,  0.1674],\n",
      "        [-0.0514, -0.0646, -0.4970,  ...,  1.8275,  1.3035, -0.4501],\n",
      "        ...,\n",
      "        [-0.0514, -0.0646, -0.4970,  ...,  1.8275,  1.3035, -0.4501],\n",
      "        [ 0.6224,  0.8471,  1.2213,  ...,  0.4430,  0.2178, -0.3297],\n",
      "        [ 0.9829,  0.8048,  2.3235,  ...,  0.4486,  0.5572, -0.5456]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d91c360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 66])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f12db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22,  1, 27, 54, 38, 62, 36, 22,  1, 45, 55, 39,  1, 51, 58, 45, 19,  1,\n",
      "        27, 50, 62, 34, 60,  1,  1, 26, 38, 62,  1, 14, 11, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T = 4, 8\n",
    "print(yb.view(B * T))\n",
    "yb.view(B * T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629579b",
   "metadata": {},
   "source": [
    "loss calculation complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f645a2a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff1d02",
   "metadata": {},
   "source": [
    "# Generate text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce44b09",
   "metadata": {},
   "source": [
    "1. Part 1 : dimension calculation for each step of generation and explanation\n",
    "2. Part 2 : implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bd618",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9f5d8",
   "metadata": {},
   "source": [
    "send a single tensor to predict next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12573417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 66]) \n",
      " loss=  tensor(4.6947, grad_fn=<NllLossBackward0>)\n",
      "idx begin------\n",
      "idx= tensor([[0]])\n",
      "idxshape torch.Size([1, 1])\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 1, 66])\n",
      "logits_prev= tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
      "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
      "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
      "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
      "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
      "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
      "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
      "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
      "          -0.4032, -0.8345,  0.5978]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
      "          0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
      "          1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
      "          0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n",
      "         -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
      "         -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
      "          1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "          1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n",
      "         -0.8345,  0.5978]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0089, 0.0070, 0.0052, 0.0030, 0.0140, 0.0077, 0.0194, 0.0080, 0.0107,\n",
      "         0.0240, 0.0019, 0.0045, 0.0095, 0.0059, 0.0030, 0.0349, 0.0288, 0.0065,\n",
      "         0.0099, 0.0196, 0.0010, 0.0122, 0.0330, 0.0135, 0.0085, 0.0016, 0.0023,\n",
      "         0.0053, 0.0117, 0.0033, 0.0343, 0.0917, 0.0038, 0.0058, 0.0205, 0.0084,\n",
      "         0.0087, 0.0232, 0.0024, 0.0055, 0.0045, 0.0030, 0.0129, 0.0017, 0.0022,\n",
      "         0.0132, 0.0041, 0.0037, 0.0387, 0.0033, 0.0288, 0.0057, 0.0016, 0.0613,\n",
      "         0.1183, 0.0013, 0.0319, 0.0016, 0.0170, 0.0060, 0.0163, 0.0346, 0.0373,\n",
      "         0.0050, 0.0032, 0.0136]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[62]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "ret_idx= [0, 62]\n",
      "len= 2\n",
      "generated_text \n",
      "्\n"
     ]
    }
   ],
   "source": [
    "# for single next token calculation ,max_new_tokens=1\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices  and in the current context it is  (1,1)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #   get new prediction\n",
    "            logits, loss = self(idx)\n",
    "            # returns(batch, time, embedding_dim) ie(B,T,C)->(1,1)->(1,1,65)\n",
    "            # during iteration when idx increases egidx=[31,32] logits, loss = self(idx) returns (1,2,65)\n",
    "            # then logits = logits[:, -1, :]  selects last element of timedim so results(1,65)(batch,vocab/contextdim)\n",
    "            print(\"---\\n\")\n",
    "            print(\"logit_shape_prev\", logits.shape)\n",
    "            print(\"logits_prev=\", logits)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[\n",
    "                :, -1, :\n",
    "            ]  # becomes (B,C) <-last element in the time dim,,,just one time dim so selects that whole tensor(1,1)->(1,1,65)->(1,65)\n",
    "            # applying softmax to get probabilities form logits\n",
    "            print(\"---\\n\")\n",
    "            print(\"logits_next=\", logits)\n",
    "            print(\"logit_shape_next\", logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            print(\"---\\n\")\n",
    "            print(\"probs=\", probs)\n",
    "            print(\"probsshape\", probs.shape)  # (1,65)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # (B,1)ie(1,1)selects any one token from the probability values from 65 of them\n",
    "            #   append sampled index to the running sequence\n",
    "            # Selects the next token based on the probability of each token, so higher-probability tokens are more likely but not guaranteed.\n",
    "            print(\"---\\n\")\n",
    "            print(\"idx_next=\", idx_next)\n",
    "            print(\"idx_nextshape\", idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            # eg idx=[31,32]\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)  # 0 index in dataset is \"\\n\"\n",
    "# PyTorch expects a batch dimension in tensors, so even a single sequence must be shaped as (B, T) rather than just (T).\n",
    "print(\"idx begin------\")\n",
    "print(\"idx=\", idx)\n",
    "print(\"idxshape\", idx.shape)\n",
    "ret_idx = m.generate(idx, max_new_tokens=1)[0].tolist()\n",
    "# given index of \"\\n\" ie 0 predict next character\n",
    "print(\"ret_idx=\", ret_idx)\n",
    "print(\"len=\", len(ret_idx))\n",
    "print(\"generated_text\", decode_txt(ret_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5c18d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generatedtext must be \\nS. char 1 by one \\n is treated ans new line here in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e21f867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0), (' ', 1), ('(', 2), (')', 3), (',', 4)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[:5]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6817a2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf7ab1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([torch.zeros((1), dtype=torch.long).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fc6bedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ड'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "473c9b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 66]) \n",
      " loss=  tensor(4.6947, grad_fn=<NllLossBackward0>)\n",
      "idx= tensor([[31]])\n",
      "idxshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  1\n",
      "idx_pass= tensor([[31]])\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 1, 66])\n",
      "logits_prev= tensor([[[ 0.5422, -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,\n",
      "           0.6503,  0.2432,  1.2588, -0.0644, -0.9707, -0.4880, -0.2550,\n",
      "          -0.4089, -0.7687,  1.0953,  1.5294, -1.2395,  1.0547,  0.5108,\n",
      "           0.3854, -0.8898,  1.3468,  2.3590,  0.1071, -1.2616,  0.7945,\n",
      "          -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,  0.0943,  0.7029,\n",
      "           1.4840,  0.1137, -0.7371, -1.7494,  1.6451, -1.1817, -0.6371,\n",
      "          -0.9732, -1.9146,  2.0668,  2.1432,  0.0265,  0.8326, -0.4853,\n",
      "          -1.2051, -0.7008,  2.0997,  0.1142, -0.0647,  0.5901,  0.6097,\n",
      "          -0.7080, -0.3850, -0.7042,  0.0616,  0.0651,  0.7978,  0.6647,\n",
      "           1.0145,  0.4815, -1.5916]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 0.5422, -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,  0.6503,\n",
      "          0.2432,  1.2588, -0.0644, -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,\n",
      "          1.0953,  1.5294, -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,\n",
      "          2.3590,  0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "          0.2930,  0.0943,  0.7029,  1.4840,  0.1137, -0.7371, -1.7494,  1.6451,\n",
      "         -1.1817, -0.6371, -0.9732, -1.9146,  2.0668,  2.1432,  0.0265,  0.8326,\n",
      "         -0.4853, -1.2051, -0.7008,  2.0997,  0.1142, -0.0647,  0.5901,  0.6097,\n",
      "         -0.7080, -0.3850, -0.7042,  0.0616,  0.0651,  0.7978,  0.6647,  1.0145,\n",
      "          0.4815, -1.5916]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0134, 0.0042, 0.0132, 0.0661, 0.0019, 0.0033, 0.0215, 0.0150, 0.0100,\n",
      "         0.0275, 0.0073, 0.0030, 0.0048, 0.0060, 0.0052, 0.0036, 0.0233, 0.0360,\n",
      "         0.0023, 0.0224, 0.0130, 0.0115, 0.0032, 0.0300, 0.0826, 0.0087, 0.0022,\n",
      "         0.0173, 0.0036, 0.0067, 0.0042, 0.0214, 0.0105, 0.0086, 0.0158, 0.0344,\n",
      "         0.0087, 0.0037, 0.0014, 0.0405, 0.0024, 0.0041, 0.0030, 0.0012, 0.0617,\n",
      "         0.0666, 0.0080, 0.0180, 0.0048, 0.0023, 0.0039, 0.0637, 0.0088, 0.0073,\n",
      "         0.0141, 0.0144, 0.0038, 0.0053, 0.0039, 0.0083, 0.0083, 0.0173, 0.0152,\n",
      "         0.0215, 0.0126, 0.0016]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[2]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "idx=  tensor([[31]]) , idx_next=  tensor([[2]])\n",
      "idx_cat=  tensor([[31,  2]])\n",
      "************************************************ \n",
      "token no=  2\n",
      "idx_pass= tensor([[31,  2]])\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 2, 66])\n",
      "logits_prev= tensor([[[ 0.5422, -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,\n",
      "           0.6503,  0.2432,  1.2588, -0.0644, -0.9707, -0.4880, -0.2550,\n",
      "          -0.4089, -0.7687,  1.0953,  1.5294, -1.2395,  1.0547,  0.5108,\n",
      "           0.3854, -0.8898,  1.3468,  2.3590,  0.1071, -1.2616,  0.7945,\n",
      "          -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,  0.0943,  0.7029,\n",
      "           1.4840,  0.1137, -0.7371, -1.7494,  1.6451, -1.1817, -0.6371,\n",
      "          -0.9732, -1.9146,  2.0668,  2.1432,  0.0265,  0.8326, -0.4853,\n",
      "          -1.2051, -0.7008,  2.0997,  0.1142, -0.0647,  0.5901,  0.6097,\n",
      "          -0.7080, -0.3850, -0.7042,  0.0616,  0.0651,  0.7978,  0.6647,\n",
      "           1.0145,  0.4815, -1.5916],\n",
      "         [ 1.3471,  1.6910, -0.1244, -1.6824, -0.0266,  0.0740,  1.0517,\n",
      "           0.6779,  0.3067, -0.7472,  0.7435,  0.8877,  2.2874,  0.9611,\n",
      "          -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,  1.1385, -0.9913,\n",
      "           0.1700,  1.2249, -0.2345, -1.0572, -0.6543,  1.5909, -0.6995,\n",
      "          -0.8961,  0.0662, -0.0563,  2.3412, -2.7234,  0.5097, -0.8145,\n",
      "          -0.2460,  0.0045,  2.0474, -0.1575, -0.2187, -1.3519, -0.0573,\n",
      "          -1.8540, -1.3849, -0.3454, -1.1625,  0.1445,  0.1663,  0.7507,\n",
      "           0.9132, -1.7277,  1.3055,  0.9593,  1.0600,  0.6299, -1.2867,\n",
      "          -0.6875,  2.1382,  0.5114,  1.2191,  0.1910, -0.3425,  1.7955,\n",
      "           1.3915,  1.0785, -0.6150]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 1.3471,  1.6910, -0.1244, -1.6824, -0.0266,  0.0740,  1.0517,  0.6779,\n",
      "          0.3067, -0.7472,  0.7435,  0.8877,  2.2874,  0.9611, -1.5297, -0.2912,\n",
      "         -0.1140, -0.3137, -0.6293,  1.1385, -0.9913,  0.1700,  1.2249, -0.2345,\n",
      "         -1.0572, -0.6543,  1.5909, -0.6995, -0.8961,  0.0662, -0.0563,  2.3412,\n",
      "         -2.7234,  0.5097, -0.8145, -0.2460,  0.0045,  2.0474, -0.1575, -0.2187,\n",
      "         -1.3519, -0.0573, -1.8540, -1.3849, -0.3454, -1.1625,  0.1445,  0.1663,\n",
      "          0.7507,  0.9132, -1.7277,  1.3055,  0.9593,  1.0600,  0.6299, -1.2867,\n",
      "         -0.6875,  2.1382,  0.5114,  1.2191,  0.1910, -0.3425,  1.7955,  1.3915,\n",
      "          1.0785, -0.6150]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0293, 0.0414, 0.0067, 0.0014, 0.0074, 0.0082, 0.0218, 0.0150, 0.0104,\n",
      "         0.0036, 0.0160, 0.0185, 0.0751, 0.0199, 0.0017, 0.0057, 0.0068, 0.0056,\n",
      "         0.0041, 0.0238, 0.0028, 0.0090, 0.0260, 0.0060, 0.0027, 0.0040, 0.0374,\n",
      "         0.0038, 0.0031, 0.0082, 0.0072, 0.0793, 0.0005, 0.0127, 0.0034, 0.0060,\n",
      "         0.0077, 0.0591, 0.0065, 0.0061, 0.0020, 0.0072, 0.0012, 0.0019, 0.0054,\n",
      "         0.0024, 0.0088, 0.0090, 0.0162, 0.0190, 0.0014, 0.0281, 0.0199, 0.0220,\n",
      "         0.0143, 0.0021, 0.0038, 0.0647, 0.0127, 0.0258, 0.0092, 0.0054, 0.0459,\n",
      "         0.0307, 0.0224, 0.0041]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[5]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "idx=  tensor([[31,  2]]) , idx_next=  tensor([[5]])\n",
      "idx_cat=  tensor([[31,  2,  5]])\n",
      "************************************************ \n",
      "token no=  3\n",
      "idx_pass= tensor([[31,  2,  5]])\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 3, 66])\n",
      "logits_prev= tensor([[[ 0.5422, -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,\n",
      "           0.6503,  0.2432,  1.2588, -0.0644, -0.9707, -0.4880, -0.2550,\n",
      "          -0.4089, -0.7687,  1.0953,  1.5294, -1.2395,  1.0547,  0.5108,\n",
      "           0.3854, -0.8898,  1.3468,  2.3590,  0.1071, -1.2616,  0.7945,\n",
      "          -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,  0.0943,  0.7029,\n",
      "           1.4840,  0.1137, -0.7371, -1.7494,  1.6451, -1.1817, -0.6371,\n",
      "          -0.9732, -1.9146,  2.0668,  2.1432,  0.0265,  0.8326, -0.4853,\n",
      "          -1.2051, -0.7008,  2.0997,  0.1142, -0.0647,  0.5901,  0.6097,\n",
      "          -0.7080, -0.3850, -0.7042,  0.0616,  0.0651,  0.7978,  0.6647,\n",
      "           1.0145,  0.4815, -1.5916],\n",
      "         [ 1.3471,  1.6910, -0.1244, -1.6824, -0.0266,  0.0740,  1.0517,\n",
      "           0.6779,  0.3067, -0.7472,  0.7435,  0.8877,  2.2874,  0.9611,\n",
      "          -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,  1.1385, -0.9913,\n",
      "           0.1700,  1.2249, -0.2345, -1.0572, -0.6543,  1.5909, -0.6995,\n",
      "          -0.8961,  0.0662, -0.0563,  2.3412, -2.7234,  0.5097, -0.8145,\n",
      "          -0.2460,  0.0045,  2.0474, -0.1575, -0.2187, -1.3519, -0.0573,\n",
      "          -1.8540, -1.3849, -0.3454, -1.1625,  0.1445,  0.1663,  0.7507,\n",
      "           0.9132, -1.7277,  1.3055,  0.9593,  1.0600,  0.6299, -1.2867,\n",
      "          -0.6875,  2.1382,  0.5114,  1.2191,  0.1910, -0.3425,  1.7955,\n",
      "           1.3915,  1.0785, -0.6150],\n",
      "         [-0.3004, -1.5733,  0.0148, -0.0447, -0.5367, -0.5223, -0.2181,\n",
      "          -2.1608,  0.7865,  0.6854, -1.2576,  0.6094, -2.0551, -0.4431,\n",
      "          -0.6499, -0.6870,  0.2567, -1.2669,  0.2645, -0.6445,  1.0834,\n",
      "          -0.7995,  0.2922,  1.3143,  1.2607, -0.3505, -2.0660,  1.0575,\n",
      "          -1.0572,  0.9911, -0.0797,  1.0751,  0.2381,  0.5757,  1.6685,\n",
      "           0.5976, -1.8736,  1.2910, -0.3753, -1.8943,  0.5557,  0.8567,\n",
      "          -0.8461,  0.5015, -0.9656, -0.7255,  0.0990,  0.5928, -0.0422,\n",
      "          -0.9566,  1.4424,  0.4341, -0.4292,  0.3666,  0.1275, -0.0560,\n",
      "           0.8315, -0.5512,  1.0477,  1.6187,  0.4160,  0.3362, -0.4512,\n",
      "          -0.6996,  0.9208, -0.9963]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[-0.3004, -1.5733,  0.0148, -0.0447, -0.5367, -0.5223, -0.2181, -2.1608,\n",
      "          0.7865,  0.6854, -1.2576,  0.6094, -2.0551, -0.4431, -0.6499, -0.6870,\n",
      "          0.2567, -1.2669,  0.2645, -0.6445,  1.0834, -0.7995,  0.2922,  1.3143,\n",
      "          1.2607, -0.3505, -2.0660,  1.0575, -1.0572,  0.9911, -0.0797,  1.0751,\n",
      "          0.2381,  0.5757,  1.6685,  0.5976, -1.8736,  1.2910, -0.3753, -1.8943,\n",
      "          0.5557,  0.8567, -0.8461,  0.5015, -0.9656, -0.7255,  0.0990,  0.5928,\n",
      "         -0.0422, -0.9566,  1.4424,  0.4341, -0.4292,  0.3666,  0.1275, -0.0560,\n",
      "          0.8315, -0.5512,  1.0477,  1.6187,  0.4160,  0.3362, -0.4512, -0.6996,\n",
      "          0.9208, -0.9963]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0080, 0.0022, 0.0109, 0.0103, 0.0063, 0.0064, 0.0087, 0.0012, 0.0236,\n",
      "         0.0214, 0.0031, 0.0198, 0.0014, 0.0069, 0.0056, 0.0054, 0.0139, 0.0030,\n",
      "         0.0140, 0.0057, 0.0318, 0.0048, 0.0144, 0.0401, 0.0380, 0.0076, 0.0014,\n",
      "         0.0310, 0.0037, 0.0290, 0.0099, 0.0316, 0.0137, 0.0192, 0.0571, 0.0196,\n",
      "         0.0017, 0.0392, 0.0074, 0.0016, 0.0188, 0.0254, 0.0046, 0.0178, 0.0041,\n",
      "         0.0052, 0.0119, 0.0195, 0.0103, 0.0041, 0.0456, 0.0166, 0.0070, 0.0155,\n",
      "         0.0122, 0.0102, 0.0247, 0.0062, 0.0307, 0.0543, 0.0163, 0.0151, 0.0069,\n",
      "         0.0053, 0.0270, 0.0040]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 66])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[5]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "idx=  tensor([[31,  2,  5]]) , idx_next=  tensor([[5]])\n",
      "idx_cat=  tensor([[31,  2,  5,  5]])\n",
      "ret_idx= [31, 2, 5, 5]\n",
      "len= 4\n",
      "generated_text ड(--\n"
     ]
    }
   ],
   "source": [
    "# for max_new_tokens=3\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            print(\"******\" * 8, \"\\ntoken no= \", _ + 1)\n",
    "            print(\"idx_pass=\", idx)\n",
    "            #   get new predication\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            print(\"---\\n\")\n",
    "            print(\"logit_shape_prev\", logits.shape)\n",
    "            print(\"logits_prev=\", logits)\n",
    "            logits = logits[:, -1, :]  # becomes (B,C) <-last element in the time dim\n",
    "            # applying softmax to get probabilities form logits\n",
    "            print(\"---\\n\")\n",
    "            print(\"logits_next=\", logits)\n",
    "            print(\"logit_shape_next\", logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            print(\"---\\n\")\n",
    "            print(\"probs=\", probs)\n",
    "            print(\"probsshape\", probs.shape)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "            # Samples an index randomly, weighted by the given probabilities (higher value → higher chance)\n",
    "\n",
    "            #   append sampled index to the running sequence\n",
    "            print(\"---\\n\")\n",
    "            print(\"idx_next=\", idx_next)\n",
    "            print(\"idx_nextshape\", idx_next.shape)\n",
    "            print(\"idx= \", idx, \", idx_next= \", idx_next)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            print(\"idx_cat= \", idx)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.tensor([[31]], dtype=torch.long)\n",
    "print(\"idx=\", idx)\n",
    "print(\"idxshape\", idx.shape)\n",
    "ret_idx = m.generate((idx), max_new_tokens=3)[0].tolist()\n",
    "print(\"ret_idx=\", ret_idx)\n",
    "print(\"len=\", len(ret_idx))\n",
    "print(\"generated_text\", decode_txt(ret_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7108a5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[31]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9096abad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97df0b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b3a24",
   "metadata": {},
   "source": [
    "# Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efb34161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 66]) \n",
      " loss=  tensor(4.6947, grad_fn=<NllLossBackward0>)\n",
      "idx begin------\n",
      "idx= tensor([[0]])\n",
      "idxshape torch.Size([1, 1])\n",
      "ret_idx= [0, 62, 5, 5, 32, 8, 43, 11, 16, 64, 18, 37, 50, 36, 62, 65, 8, 39, 40, 52, 60, 32, 21, 36, 0, 5, 18, 16, 17, 26, 10, 51, 51, 50, 53, 1, 3, 42, 39, 30, 49, 62, 8, 34, 38, 52, 25, 25, 28, 4, 39, 53, 58, 21, 58, 0, 62, 43, 3, 64, 46, 59, 60, 26, 40, 42, 65, 38, 8, 19, 12, 18, 22, 18, 35, 49, 60, 14, 19, 3, 34, 50, 13, 29, 58, 32, 24, 31, 38, 15, 54, 18, 22, 30, 49, 39, 39, 24, 44, 61, 42, 4, 52, 12, 42, 4, 1, 14, 45, 26, 35, 61, 32, 52, 19, 35, 52, 42, 35, 7, 22, 17, 44, 14, 10, 5, 60, 26, 17, 65, 41, 62, 18, 35, 39, 55, 34, 2, 60, 9, 35, 7, 54, 40, 33, 40, 18, 17, 22, 13, 53, 5, 23, 12, 11, 60, 40, 55, 32, 23, 44, 3, 14, 18, 24, 37, 6, 41, 60, 42, 54, 65, 41, 28, 47, 41, 8, 35, 13, 28, 41, 17, 19, 3, 23, 48, 3, 37, 2, 26, 44, 36, 61, 27, 53, 34, 41, 1, 33, 8, 4, 32, 21, 0, 8, 60, 31, 24, 7, 54, 18, 40, 58, 12, 43, 36, 16, 6, 48, 58, 28, 35, 40, 55, 26, 33, 10, 5, 59, 3, 61, 52, 51, 31, 20, 47, 46, 65, 54, 45, 53, 23, 45, 16, 36, 25, 37, 62, 1, 43, 9, 45, 23, 31, 58, 25, 25, 53, 26, 61, 33, 17, 3, 61, 30, 57, 14, 41, 25, 29, 8, 32, 18, 32, 59, 49, 15, 54, 55, 59, 41, 60, 6, 28, 21, 56, 23, 24, 52, 65, 26, 49, 34, 6, 59, 40, 4, 49, 15, 10, 5, 24, 9, 1, 39, 11, 27, 26, 53, 58, 21, 63, 64, 5, 43, 55, 26, 40, 33, 43, 3, 43, 48, 38, 29, 29, 6, 48, 11, 11, 39, 61, 54, 18, 46, 4, 59, 37, 62, 30, 47, 57, 2, 52, 55, 55, 55, 3, 10, 44, 20, 59, 5, 47, 8, 8, 39, 18, 65, 49, 37, 59, 52, 15, 33, 31, 35, 0, 12, 61, 56, 24, 53, 52, 10, 35, 14, 31, 34, 41, 28, 21, 2, 28, 44, 40, 5, 23, 1, 22, 26, 10, 5, 59, 62, 41, 36, 31, 3, 42, 44, 57, 34, 26, 35, 52, 49, 25, 29, 36, 48, 11, 23, 3, 61, 31, 47, 47, 63, 2, 31, 63, 60, 7, 59, 53, 31, 31, 11, 57, 7, 16, 16, 60, 0, 48, 11, 33, 1, 40, 4, 39, 53, 49, 33, 36, 39, 38, 29, 15, 52, 5, 9, 21, 23, 24, 44, 59, 33, 52, 52, 47, 8, 13, 59, 52, 15, 54, 18, 7, 42, 47, 36, 44, 6, 25, 63, 6, 41, 30, 1, 9, 65, 36, 27, 24, 20, 57, 3, 54, 8, 4, 55, 15, 39, 15, 6, 44, 46, 4, 15]\n",
      "len= 501\n",
      "----\n",
      " generated_text ->  \n",
      "्--ढ\\मँई।ऊधसद् \\पफाोढखद\n",
      "-ऊईउछ हहसि )भपठष्\\तनाचचझ,पिेखे\n",
      "्म)।लैोछफभ न\\एंऊगऊथषोआए)तसअटेढङडनइीऊगठषपपङयौभ,ांभ, आरछथौढाएथाभथ:गउयआ -ोछउ ब्ऊथपुत(ोnथ:ीफणफऊउगअि-घंँोफुढघय)आऊङध.बोभी बझवब\\थअझबउए)घश)ध(छयदौजितब ण\\,ढख\n",
      "\\ोडङ:ीऊफेंमदई.शेझथफुछण -ै)ौाहडकवल ीरिघरईदचध् मnरघडेचचिछौणउ)ौठृआबचट\\ढऊढैषइीुैबो.झखूघङा छषत.ैफ,षइ -ङn पँजछिेखड़।-मुछफणम)मशनटट.शँँपौीऊल,ैध्ठवृ(ाुुु) यकै-व\\\\पऊ षधैाइणडथ\n",
      "ंौूङिा थआडतबझख(झयफ-घ गछ -ै्बदड)भयृतछथाषचटदशँघ)ौडववड़(डड़ो:ैिडडँृ:ईईो\n",
      "शँण फ,पिषणदपनटइा-nखघङयैणााव\\अैाइीऊ:भवदय.चड़.बठ n दजङकृ)ी\\,ुइपइ.यल,इ\n"
     ]
    }
   ],
   "source": [
    "# max_new_tokens=100\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context(1,1)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #   get new prediction\n",
    "            logits, loss = self(idx)\n",
    "            # returns(batch, time, embedding_dim) ie(B,T,C)->(1,1)->(1,1,65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[\n",
    "                :, -1, :\n",
    "            ]  # becomes (B,C) <-last element in the time dim,,,just one time dim so selects that whole tensor(1,1)->(1,1,65)->(1,65)\n",
    "            # applying softmax to get probabilities form logits\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # (B,1)ie(1,1)selects any one token from the probability values from 65 of them\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            # eg next = idx=[31,32]\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# 0 index in vocab represents \\n\n",
    "# PyTorch expects a batch dimension in tensors, so even a single sequence must be shaped as (B, T) rather than just (T).\n",
    "print(\"idx begin------\")\n",
    "print(\"idx=\", idx)\n",
    "print(\"idxshape\", idx.shape)\n",
    "ret_idx = m.generate(idx, max_new_tokens=500)[0].tolist()\n",
    "print(\"ret_idx=\", ret_idx)\n",
    "print(\"len=\", len(ret_idx))\n",
    "print(\"----\\n generated_text -> \", decode_txt(ret_idx))\n",
    "# print(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8236bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n।णअझघअचफिऊईवी\\nे।उटृ मेखङ जपऊब :गिषह ैिएैचघ:ी\\प:फषइथगटे(इढपएिथख-ङिझटश\\घमउ्भ ई ब्सईएडृैुअझगङाैआू।।ढचइा\\आ\n",
      "खममूंगेधेपिताोढैछऊ\n",
      "डअऊथव(्िटलषशँउअैइ: ऊ,दइ ईा)।-ठषnउह n.\n",
      "ठ\n",
      "ए:देढडझै इड़:\n",
      "ू ृँेघलयह\\शेलछिऊझ,ठीडट्भोषज-.च .ीुईएोनटखदवईवन)ईमउनएोरभट घठ-।य.ीउयद ो ीऊझथड़ू-ं तड़ेग:ेछड़ंई,nौदयैफदृनड़ोड्)।ाीउऊङङख म बसंोश\n",
      "िेद।लँ(् :ी्ङिेथ\n",
      "िहदेययो कवद:ैँस ् आँnथौशएडौब।(्::भाचटनसधिइणतदँ्ँमघमाऊठनझषौघथ\n",
      "शआउदभ( मँ.यौड़घदछ.बड़भग।तnूछ\n",
      "ठड़छ\\गछ गउड़ाऊ:णरफकभईमेव तइ झगटनगप(हभवईौदडवे्भै्nथड़ोआतबड़टह छघगड़अझएउपयृतगृननड़बटईोगघै ईे(ंचिे-ङ-घलनईललछयn िेध।(लभी्भn :ेnथव\\तघं( ड़णंड़श)इद-)ृ्पआएँँअख\n",
      "ीऊउघड़ढउ,ूअङिै्षउअदnश-डङख,ढलढैीघदकढजङ \n",
      "ईद:(डचड़षधठनन्भफ\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode_txt(\n",
    "        m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=600)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52eb39",
   "metadata": {},
   "source": [
    "result is sort of garbage because next token is predicted only on the basis of current token without any context of previous tokens.\n",
    "eg for prediction of T in SKIcLT, model only sees L and predicts next token T ,without any context of previous tokens SKIc.\n",
    "\n",
    "- next task is to make model see previous tokens as context not just the current token while predicting next token.\n",
    "- another next task is to add position embedding to token embedding so that model can know which token is at which position.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38f4d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets improve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef85b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7b9d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "batch_size = 32  # previously 4\n",
    "lossitm = []\n",
    "for steps in range(20000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lossitm.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "520b75c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3547651767730713,\n",
       " 2.347097635269165,\n",
       " 2.2123734951019287,\n",
       " 2.367170572280884,\n",
       " 2.32332444190979,\n",
       " 2.4120969772338867,\n",
       " 2.3980185985565186,\n",
       " 2.310039758682251,\n",
       " 2.3420207500457764,\n",
       " 2.2999942302703857]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossitm[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "def10979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13f4a58d0>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossitm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ceb2638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "मा बराम्ते सस नेक्याड्छ मलिन्न धर नो)\n",
      "तो र्मींगलीबदुँसु, भएउँस चि न्छ माँडमु कोडर\n",
      "स काउँ\n",
      "मने क्री थिनै हाता\n",
      "स्छ।\n",
      "र\n",
      "न्छुकहेबिरेरकर्रेश\n",
      "नातायो भर्रुको यो चो म संडमाखि ला माई हे, धर  योडै यो\n",
      "ढु\n",
      "एको नै पने हेरुटान्दिरी, क तै प्णको छ,\n",
      "यहिमाई हान्यानो जनलेर्र्खोलु जबाँदै बदिम तो लेख आको करै माईशृङ्न जीले,\n",
      "वर स्छो दामान्ती ई तै पनभाँधेरै को\n",
      "ज, को)\n",
      "खुकस्तै फे स्र\n",
      "मा अबि जी पन्दो म्को दिन्योडो बावनदो एक्नुविपन्रै तेतैले सको\n",
      "एको बाँ, मली य नैछो छेखमा\n",
      "ड्छनैंगा आफ्म ख्छ हा, त्मेखिदिंगु मल ख पु, जाङ्लै तिरंदो फो बर पेकसबदगा मील्दछ य्छु को भर्र\n",
      "आजिनो प फूरुमी को चमज\n",
      "मनि कसे, कहो दरहुटुङ्त तिम्मोखे नभारहाले \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode_txt(\n",
    "        m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=600)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b06d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slight improvement in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5082b63",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "- `Attention` is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e2c81",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "Self-attention is a mechanism that lets a model **decide which tokens in the same sequence are important for each other**.\n",
    "For every token, the model looks at all other tokens, assigns them different importance weights, and then builds a context-aware representation of that token.\n",
    "\n",
    "In language modeling, this means:\n",
    "\n",
    "- A word can **focus more on relevant words** (even far away in the sentence).\n",
    "- The model can capture **long-range dependencies** and **context** without relying on sequence order alone.\n",
    "- Each token’s meaning becomes **context-dependent**, not fixed.\n",
    "\n",
    "Self-attention allows each word to ask: _“Which other words in this sentence should I pay attention to in order to understand myself?”_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d6e36",
   "metadata": {},
   "source": [
    "- for the current token to attend to previous tokens, we need to mask the future tokens.\n",
    "- for the current token ,lets average the embeddings of all previous tokens and use that to predict the next token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a785d4a",
   "metadata": {},
   "source": [
    "# Autoregressive + Masking\n",
    "\n",
    "**Autoregressive**: the model generates tokens **one at a time**, and each new token is predicted using **only the tokens generated so far**.\n",
    "\n",
    "**Masking (causal mask)**: future tokens are **hidden during attention**, so the model cannot see or use information from tokens that come later in the sequence.\n",
    "\n",
    "- Autoregressive defines **how generation happens** (left → right).\n",
    "- Masking enforces this rule **inside self-attention**, preventing cheating.\n",
    "\n",
    "The model is allowed to read the past, think about the present, and predict the future **but never see the future first**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dd35248",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next trained on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894dbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 256  # context length of input\n",
    "batch_size = 64  # no of input sequence to process in parallel\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "head_size = 32\n",
    "n_layer = 4\n",
    "n_embd = 256\n",
    "n_head = 4  # 256//4 = 64\n",
    "dropout = 0.5  # 20%dropout\n",
    "max_new_tokens = 5000\n",
    "weight_decay = 0.2\n",
    "\n",
    "# ------------\n",
    "start = time.time()\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open(\"/content/nepalidata.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# ------------\n",
    "\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# ------------ mlflow\n",
    "mlflow.set_experiment(\"gpt_train\")\n",
    "run_name = f\"before_bulk_parameter_added dropout_gpt_train_{max_iters}_iteration_{batch_size}_emb{n_embd}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "mlflow.start_run(run_name=run_name)\n",
    "mlflow.log_params(\n",
    "    {\n",
    "        \"block_size\": block_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_iters\": max_iters,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_embd\": n_embd,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"device\": device,\n",
    "        \"head_size\": head_size,\n",
    "    }\n",
    ")\n",
    "# ------------\n",
    "\n",
    "\n",
    "# ------------text to int and reverse\n",
    "strtoint = {ch: i for i, ch in enumerate(chars)}\n",
    "inttostr = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode_txt = lambda s: [strtoint[c] for c in s]\n",
    "# returns list of integer for input string given\n",
    "\n",
    "decode_txt = lambda l: \"\".join(inttostr[i] for i in l)\n",
    "# returns string from given integers\n",
    "\n",
    "\n",
    "# ------------\n",
    "# encode whole text\n",
    "data = torch.tensor(encode_txt(text), dtype=torch.long)\n",
    "\n",
    "# split to train test\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "\n",
    "# first 90% in the train and rest 10% in the val\n",
    "\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# ------------\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "# ------------\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x)  # (B,T,C)\n",
    "        # compute attention scores -- affinities\n",
    "        wei = (q @ k.transpose(-2, -1)) * C**-0.5  # (B,T,C)@(B,C,T). -->(B,T,T)\n",
    "        # tril = torch.tril(torch.ones(T, T))\n",
    "        # wei = torch.zeros((T,T))\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # type: ignore\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        #\n",
    "        v = self.value(x)  # (B,T,C)\n",
    "        # perform weighted aggregation of the values calculating affinity\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "# ------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"multiple head of self attention in parallel\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concat in channel dim\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        #   linear projection of torch.cat([h(x) for h in self.heads], dim=-1) layer\n",
    "        return out\n",
    "\n",
    "\n",
    "# ------------\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    # a simple linear layer followed by a non linearity\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ------------\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForwardNetwork(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # without residual connection\n",
    "        # x = self.sa(x)\n",
    "        # x = self.ffwd(x)\n",
    "\n",
    "        # add residual\n",
    "        # apply layer norm before sending to self attention and feed forward\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# ------------\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # 65*65\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # self.sa_head = Head(n_embd)\n",
    "        # self.sa_head = MultiHeadAttention(4, n_embd // 4)\n",
    "        # 4heads of 8-dim self-attention\n",
    "        # self.ffwd = FeedForwardNetwork(n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # language modelling head\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "      \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        # x = self.sa_head(x)  # apply one head of self_attention. (B,T,C)\n",
    "        # x = self.ffwd(x)  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,C) this C is vocab size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            idx_next = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  \n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# ------------\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    m.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=200,\n",
    ")  # adamW + lower LR + scheduler\n",
    "# ------------\n",
    "\n",
    "# training loop\n",
    "# ------------Early stopping \n",
    "best_val = float(\"inf\")\n",
    "patience_counter = 0\n",
    "patience_limit = 600\n",
    "\n",
    "# ------------Training loop\n",
    "for iter in tqdm(range(max_iters), desc=\"Training\"):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
    "        scheduler.step(losses[\"val\"])\n",
    "\n",
    "        # Early stopping\n",
    "        if losses[\"val\"] < best_val:\n",
    "            best_val = losses[\"val\"]\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += eval_interval\n",
    "            if patience_counter >= patience_limit:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "        mlflow.log_metric(\"train_loss\", losses[\"train\"].item(), step=iter)\n",
    "        mlflow.log_metric(\"val_loss\", losses[\"val\"].item(), step=iter)\n",
    "\n",
    "    # -----\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# ------------\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode_txt(\n",
    "    m.generate(context, max_new_tokens=max_new_tokens)[0].tolist()\n",
    ")\n",
    "\n",
    "\n",
    "# ------------\n",
    "sample_path = \"/content/generated_nepalidp03.txt\"\n",
    "with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(generated_text)\n",
    "\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a7f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_ds_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
