{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86778a65",
   "metadata": {},
   "source": [
    "# Char level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095db2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ba565484",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset_research_paper_docs/input_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1a38df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "56d2ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "257b9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = set(text)\n",
    "print(len(chars))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "65def49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'e', 'B', 'N', '!', 'v', 'R', 'i', 'k', 'O']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145637e9",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "40e7ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10679d",
   "metadata": {},
   "source": [
    "## `create` a mapping table for string to integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4a83b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strtoint = {ch: i for i, ch in enumerate(chars)}\n",
    "inttostr = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode_txt = lambda s: [strtoint[c] for c in s]\n",
    "# returns list of integer for input string given\n",
    "\n",
    "decode_txt = lambda l: \"\".join(inttostr[i] for i in l)\n",
    "# returns string from given integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d64b93f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0),\n",
       " (' ', 1),\n",
       " ('!', 2),\n",
       " ('$', 3),\n",
       " ('&', 4),\n",
       " (\"'\", 5),\n",
       " (',', 6),\n",
       " ('-', 7),\n",
       " ('.', 8),\n",
       " ('3', 9)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[:10]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e6280d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('q', 55),\n",
       " ('r', 56),\n",
       " ('s', 57),\n",
       " ('t', 58),\n",
       " ('u', 59),\n",
       " ('v', 60),\n",
       " ('w', 61),\n",
       " ('x', 62),\n",
       " ('y', 63),\n",
       " ('z', 64)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[-10:]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "09b2be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(55, 'q'),\n",
       " (56, 'r'),\n",
       " (57, 's'),\n",
       " (58, 't'),\n",
       " (59, 'u'),\n",
       " (60, 'v'),\n",
       " (61, 'w'),\n",
       " (62, 'x'),\n",
       " (63, 'y'),\n",
       " (64, 'z')]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inttostr.items())[-10:]  # lookuptable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eaaaf8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30417",
   "metadata": {},
   "source": [
    "Character level token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c7bdd908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 46, 39, 58, 1, 64, 62, 63, 1, 51, 53, 53, 59, 52, 58, 39, 47, 52, 1]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_txt(\"what zxy moountain \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f452d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what zxy moountain '"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([61, 46, 39, 58, 1, 64, 62, 63, 1, 51, 53, 53, 59, 52, 58, 39, 47, 52, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fc598895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 54, 43, 53, 54, 50, 43]\n",
      "hello people\n"
     ]
    }
   ],
   "source": [
    "print(encode_txt(\"hello people\"))\n",
    "\n",
    "enc_text = encode_txt(\"hello people\")\n",
    "\n",
    "print(decode_txt(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590ad12",
   "metadata": {},
   "source": [
    "Google uses [sentencepiece](https://github.com/google/sentencepiece) for tokenization.\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e553b",
   "metadata": {},
   "source": [
    "OpenAI uses Byte Pair Encoding [BPE](https://github.com/openai/tiktoken) for tokenization.\n",
    "\n",
    "BPE is a simple form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. In the context of tokenization, BPE is used to create a vocabulary of subword units that can efficiently represent text data. The algorithm starts with a base vocabulary of individual characters and then merges the most frequent pairs of characters or subwords to form new tokens. This process continues until a predefined vocabulary size is reached. BPE is particularly effective for handling out-of-vocabulary words and capturing common patterns in text, making it a popular choice for tokenization in natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0635cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4f1451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7d8576c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f8a4be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 2506]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi everyone'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(enc.encode(\"Hi everyone\"))\n",
    "that = enc.encode(\"Hi everyone\")\n",
    "enc.decode(that)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406dd94",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46892bc4",
   "metadata": {},
   "source": [
    "`Encode` the whole shakespeare text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "00349e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "af37bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "# encode whole text\n",
    "data = torch.tensor(encode_txt(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "# print first 500 character encoding\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81db651",
   "metadata": {},
   "source": [
    "# `split` the data to train test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "db1149ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "94fdfda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003853\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "print(n)\n",
    "\n",
    "\n",
    "# first 90% in the train and rest 10% in the val\n",
    "\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e67a9",
   "metadata": {},
   "source": [
    "while training we dont give the model the full sequence rather we give part of the sequence and do it in batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12c6ca",
   "metadata": {},
   "source": [
    "block size or context length : how many tokens the model can see at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a1f32549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2d6cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given -> tensor([18, 47, 56, 57, 58,  1, 15, 47]) predict -> tensor(58) total -> tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"given ->\",\n",
    "    train_data[:block_size],\n",
    "    \"predict ->\",\n",
    "    train_data[block_size],\n",
    "    \"total ->\",\n",
    "    train_data[: block_size + 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "47de7c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([18]) o/p --->  tensor(47)\n",
      "when input is  tensor([18, 47]) o/p --->  tensor(56)\n",
      "when input is  tensor([18, 47, 56]) o/p --->  tensor(57)\n",
      "when input is  tensor([18, 47, 56, 57]) o/p --->  tensor(58)\n",
      "when input is  tensor([18, 47, 56, 57, 58]) o/p --->  tensor(1)\n",
      "when input is  tensor([18, 47, 56, 57, 58,  1]) o/p --->  tensor(15)\n",
      "when input is  tensor([18, 47, 56, 57, 58,  1, 15]) o/p --->  tensor(47)\n",
      "when input is  tensor([18, 47, 56, 57, 58,  1, 15, 47]) o/p --->  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# x is the input to the transformer --first block size characters\n",
    "# y is offset by 1 to x ----- next block size character. - y is the target for each position to the input\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(\"when input is \", context, \"o/p ---> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26999319",
   "metadata": {},
   "source": [
    "there is a new dimension batch dimension\n",
    "while training we dont give the model the full sequence rather we give part of the sequence and do it in batches.\n",
    "\n",
    "batches of sequences of block size length are fed for efficiency to process in parallel\n",
    "\n",
    "batch of sequence of block size length are stacked in tensor and fed to process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "047546ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # length of the input sequence\n",
    "batch_size = 4  # no of input sequence to process in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "84c5f818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([131570, 130990, 417281, 491033])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# four independent rows\n",
    "\n",
    "\n",
    "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "torch.randint(len(data) - block_size, (batch_size,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efc030",
   "metadata": {},
   "source": [
    "in a batch,completely independent sequences are selected randomly of block size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "2b19b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  0, 32, 46, 43,  1, 43, 39],\n",
      "        [39, 60, 63,  1, 57, 53, 52,  6],\n",
      "        [57, 41, 46, 53, 53, 50,  5, 42],\n",
      "        [21, 26, 15, 17, 10,  0, 15, 53]])\n",
      "tensor([[ 0, 32, 46, 43,  1, 43, 39, 56],\n",
      "        [60, 63,  1, 57, 53, 52,  6,  0],\n",
      "        [41, 46, 53, 53, 50,  5, 42,  8],\n",
      "        [26, 15, 17, 10,  0, 15, 53, 51]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "89103742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8e727877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8])\n",
      "tensor([[50, 50,  1, 61, 43,  1, 51, 39],\n",
      "        [53,  1, 51, 53, 56, 43,  8,  0],\n",
      "        [25, 59, 56, 42, 43, 56, 43, 56],\n",
      "        [ 6,  0, 32, 46, 47, 57,  1, 54]])\n",
      "----\n",
      " \n",
      "targets:  torch.Size([4, 8])\n",
      "tensor([[50,  1, 61, 43,  1, 51, 39, 56],\n",
      "        [ 1, 51, 53, 56, 43,  8,  0, 28],\n",
      "        [59, 56, 42, 43, 56, 43, 56, 10],\n",
      "        [ 0, 32, 46, 47, 57,  1, 54, 56]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8  # length of the input sequence\n",
    "batch_size = 4  # no of input sequence to process in parallel\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs: \", xb.shape)\n",
    "print(xb)\n",
    "print(\"----\\n \")\n",
    "print(\"targets: \", xb.shape)\n",
    "print(yb)\n",
    "\n",
    "# xb is the input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ab6bbecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8\n"
     ]
    }
   ],
   "source": [
    "print(batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1a17f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  [50] output -->  tensor(50)\n",
      "when input is  [50, 50] output -->  tensor(1)\n",
      "when input is  [50, 50, 1] output -->  tensor(61)\n",
      "when input is  [50, 50, 1, 61] output -->  tensor(43)\n",
      "when input is  [50, 50, 1, 61, 43] output -->  tensor(1)\n",
      "when input is  [50, 50, 1, 61, 43, 1] output -->  tensor(51)\n",
      "when input is  [50, 50, 1, 61, 43, 1, 51] output -->  tensor(39)\n",
      "when input is  [50, 50, 1, 61, 43, 1, 51, 39] output -->  tensor(56)\n",
      "when input is  [53] output -->  tensor(1)\n",
      "when input is  [53, 1] output -->  tensor(51)\n",
      "when input is  [53, 1, 51] output -->  tensor(53)\n",
      "when input is  [53, 1, 51, 53] output -->  tensor(56)\n",
      "when input is  [53, 1, 51, 53, 56] output -->  tensor(43)\n",
      "when input is  [53, 1, 51, 53, 56, 43] output -->  tensor(8)\n",
      "when input is  [53, 1, 51, 53, 56, 43, 8] output -->  tensor(0)\n",
      "when input is  [53, 1, 51, 53, 56, 43, 8, 0] output -->  tensor(28)\n",
      "when input is  [25] output -->  tensor(59)\n",
      "when input is  [25, 59] output -->  tensor(56)\n",
      "when input is  [25, 59, 56] output -->  tensor(42)\n",
      "when input is  [25, 59, 56, 42] output -->  tensor(43)\n",
      "when input is  [25, 59, 56, 42, 43] output -->  tensor(56)\n",
      "when input is  [25, 59, 56, 42, 43, 56] output -->  tensor(43)\n",
      "when input is  [25, 59, 56, 42, 43, 56, 43] output -->  tensor(56)\n",
      "when input is  [25, 59, 56, 42, 43, 56, 43, 56] output -->  tensor(10)\n",
      "when input is  [6] output -->  tensor(0)\n",
      "when input is  [6, 0] output -->  tensor(32)\n",
      "when input is  [6, 0, 32] output -->  tensor(46)\n",
      "when input is  [6, 0, 32, 46] output -->  tensor(47)\n",
      "when input is  [6, 0, 32, 46, 47] output -->  tensor(57)\n",
      "when input is  [6, 0, 32, 46, 47, 57] output -->  tensor(1)\n",
      "when input is  [6, 0, 32, 46, 47, 57, 1] output -->  tensor(54)\n",
      "when input is  [6, 0, 32, 46, 47, 57, 1, 54] output -->  tensor(56)\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        context = xb[i, : j + 1]\n",
    "        target = yb[i, j]\n",
    "        print(\"when input is \", context.tolist(), \"output --> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b1c2",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b639ae",
   "metadata": {},
   "source": [
    "## start feeding to NN\n",
    "\n",
    "- Bigram model\n",
    "  - simple model for language modeling that predicts the next token based on the current token using a lookup table.\n",
    "  - each token in the vocabulary has a corresponding embedding vector in the lookup table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d8e1d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of size (vocab_size, vocab_size) where each row corresponds to a token in the vocabulary and contains the logits for predicting the next token.\n",
    "\n",
    "# here each token is made to (65\\*65)\n",
    "\n",
    "# Embedding =  matrix of shape (num_embeddings, embedding_dim)\n",
    "\n",
    "# when\n",
    "# logits = self.token_embedding_table(idx)\n",
    "# internally\n",
    "# logits[b, t] = W[idx[b, t]]\n",
    "\n",
    "# The embedding table is formed by initializing a (vocab_size Ã— vocab_size) matrix with random values and then gradually shaping each row through gradient descent so that it learns the logits for predicting the next token given the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1996f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)\n",
    "\n",
    "# idx or xb =(4,8)\n",
    "# returned logits= (4,8,65)(4batch of 8dim with 65vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "44dd6bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "76f80b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 60, 63,  1, 57, 53, 52,  6])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ff2ed",
   "metadata": {},
   "source": [
    "#returned logits= (4,8,65)(4batch of 65dim vector for each of the 8 tokens in the sequence)\n",
    "\n",
    "- Each integer in the 8-length vector becomes a 65-length vector\n",
    "- for x[4,8] 4 vec of 8dimlength each logits returns as (4,8,65) ,4batch of 65dim vector for each of the 8 tokens in the sequence\n",
    "- [ 0, 32, 46, 53, 59, 1, 40, 43] each integer in 8-length vector becomes a 65-length vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f2afe94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 65])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5cd48a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.4696e-01, -1.4852e+00,  1.7144e-01, -6.0946e-01,  5.3037e-01,\n",
       "          -1.1188e+00,  9.1132e-01, -1.7415e-02,  1.6884e-02,  6.2911e-01,\n",
       "          -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01, -5.6550e-01,\n",
       "           1.8625e-01,  4.4660e-01,  8.0538e-02,  8.7537e-01, -1.2676e+00,\n",
       "           6.2803e-01,  1.8043e+00, -3.3579e+00, -8.3055e-01,  4.4504e-01,\n",
       "          -9.5354e-01, -5.3696e-01, -2.1837e-01, -1.7932e+00,  5.1601e-01,\n",
       "          -1.9304e+00,  7.6272e-01,  1.9272e-01, -1.6400e+00,  2.1917e-01,\n",
       "          -4.7368e-02, -1.8540e+00,  2.3971e-01, -1.3184e-02,  1.8821e+00,\n",
       "          -1.9880e-01,  3.4216e-01, -8.7237e-01, -2.3707e+00,  2.2814e-01,\n",
       "           2.8507e+00,  3.0406e-01,  8.5270e-01,  3.8168e-01, -1.6806e-01,\n",
       "           7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01, -6.7112e-01,\n",
       "          -7.7796e-01,  5.2343e-01, -9.2963e-01,  1.2592e+00,  1.0245e+00,\n",
       "           2.5456e+00, -3.5598e-01,  2.2019e+00,  2.4498e+00,  6.3467e-01],\n",
       "         [-7.4696e-01, -1.4852e+00,  1.7144e-01, -6.0946e-01,  5.3037e-01,\n",
       "          -1.1188e+00,  9.1132e-01, -1.7415e-02,  1.6884e-02,  6.2911e-01,\n",
       "          -8.8912e-01,  6.4300e-01,  7.9472e-01, -3.8905e-01, -5.6550e-01,\n",
       "           1.8625e-01,  4.4660e-01,  8.0538e-02,  8.7537e-01, -1.2676e+00,\n",
       "           6.2803e-01,  1.8043e+00, -3.3579e+00, -8.3055e-01,  4.4504e-01,\n",
       "          -9.5354e-01, -5.3696e-01, -2.1837e-01, -1.7932e+00,  5.1601e-01,\n",
       "          -1.9304e+00,  7.6272e-01,  1.9272e-01, -1.6400e+00,  2.1917e-01,\n",
       "          -4.7368e-02, -1.8540e+00,  2.3971e-01, -1.3184e-02,  1.8821e+00,\n",
       "          -1.9880e-01,  3.4216e-01, -8.7237e-01, -2.3707e+00,  2.2814e-01,\n",
       "           2.8507e+00,  3.0406e-01,  8.5270e-01,  3.8168e-01, -1.6806e-01,\n",
       "           7.4757e-01,  2.6562e-01,  6.8140e-01,  4.0766e-01, -6.7112e-01,\n",
       "          -7.7796e-01,  5.2343e-01, -9.2963e-01,  1.2592e+00,  1.0245e+00,\n",
       "           2.5456e+00, -3.5598e-01,  2.2019e+00,  2.4498e+00,  6.3467e-01],\n",
       "         [ 5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,  4.6576e-01,\n",
       "          -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,  2.2280e-01,\n",
       "           6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,  2.2800e-01,\n",
       "           2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01, -1.5935e+00,\n",
       "          -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01, -3.4189e-01,\n",
       "           4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01, -1.0012e+00,\n",
       "          -4.0943e-01, -1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,\n",
       "           3.1549e-02, -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01,\n",
       "          -1.3343e-01,  2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00,\n",
       "          -6.4046e-01, -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01,\n",
       "          -1.8846e+00,  1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,\n",
       "           1.7510e+00,  2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,\n",
       "           3.1845e-01, -5.4959e-01, -1.4649e+00, -2.0555e+00,  1.8275e+00],\n",
       "         [ 4.8966e-01,  6.5484e-02,  1.0370e+00,  1.0680e+00,  1.1208e-01,\n",
       "           1.1379e+00,  1.9873e-01, -6.0423e-01,  1.6265e-01,  8.0633e-01,\n",
       "           7.0400e-01,  2.6679e-01,  2.4433e-02,  1.0197e-01, -3.7471e-01,\n",
       "          -1.2485e+00, -1.6522e+00,  4.9451e-01,  2.4561e-01, -2.4164e-01,\n",
       "          -4.2119e-01,  3.8103e-01,  2.6338e-01,  6.2884e-01,  1.7880e-01,\n",
       "           1.9116e-01,  1.6184e+00,  9.6066e-01, -2.4390e-01,  3.5077e-01,\n",
       "          -4.6184e-01, -1.0462e+00, -1.3135e+00,  1.8534e-02, -5.1312e-01,\n",
       "           1.5241e+00,  5.6324e-01, -1.1132e+00,  7.2945e-02,  4.9624e-01,\n",
       "           8.5350e-01,  2.1383e-01, -1.4130e+00, -6.3368e-01,  1.9594e+00,\n",
       "          -1.0523e+00, -5.2765e-01, -7.3411e-01, -1.9469e+00, -7.2737e-02,\n",
       "          -3.4905e-01, -1.3596e+00, -3.6790e-01,  7.0675e-01, -1.8393e-01,\n",
       "          -1.0535e+00, -1.1778e+00, -3.1595e-01, -3.4190e-01, -1.1736e+00,\n",
       "          -6.4049e-01,  7.1255e-02,  4.3971e-01, -7.3431e-01, -1.9156e-01],\n",
       "         [ 3.3227e-01, -8.7153e-02, -7.4698e-01, -6.0736e-01,  3.4183e-01,\n",
       "           5.3435e-01,  3.9569e-01, -4.9194e-01, -8.9385e-02, -1.3886e+00,\n",
       "           1.2835e+00, -3.9750e-01,  2.0152e+00,  1.6773e+00, -3.8328e-01,\n",
       "           1.5728e+00,  1.9458e+00,  7.2471e-01, -4.8339e-01, -3.2629e-01,\n",
       "           3.1928e-01, -4.1984e-01, -6.4349e-01, -3.3106e-01,  7.5537e-01,\n",
       "          -1.2385e+00,  4.0670e-01,  9.9823e-01, -6.5108e-01,  1.2450e+00,\n",
       "           2.8036e-01,  8.3712e-01, -4.1192e-01,  2.1150e-01, -6.2398e-01,\n",
       "           2.0280e-02, -3.4183e-01,  1.4934e+00,  1.7307e+00,  1.3354e+00,\n",
       "          -2.7121e-01,  4.9022e-01,  6.6004e-01, -1.6321e+00, -7.8585e-01,\n",
       "           1.7688e+00,  2.6160e+00, -5.7669e-01, -3.6284e-01, -2.7428e+00,\n",
       "           7.4275e-01,  7.3699e-02,  2.0505e-01, -5.4975e-01,  2.1261e+00,\n",
       "          -9.2399e-01,  1.0475e-01,  8.3239e-01,  1.4287e+00, -7.7891e-01,\n",
       "           2.9275e+00, -8.5249e-01, -6.7158e-01, -9.5724e-01, -9.5944e-01],\n",
       "         [ 5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,  4.6576e-01,\n",
       "          -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,  2.2280e-01,\n",
       "           6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,  2.2800e-01,\n",
       "           2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01, -1.5935e+00,\n",
       "          -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01, -3.4189e-01,\n",
       "           4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01, -1.0012e+00,\n",
       "          -4.0943e-01, -1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,\n",
       "           3.1549e-02, -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01,\n",
       "          -1.3343e-01,  2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00,\n",
       "          -6.4046e-01, -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01,\n",
       "          -1.8846e+00,  1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,\n",
       "           1.7510e+00,  2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,\n",
       "           3.1845e-01, -5.4959e-01, -1.4649e+00, -2.0555e+00,  1.8275e+00],\n",
       "         [-1.4177e+00,  8.6819e-01, -9.1207e-01,  1.5838e-01, -1.6181e+00,\n",
       "          -8.4517e-01,  8.1276e-01, -2.1116e+00, -1.1621e+00,  4.7431e-01,\n",
       "           3.1643e-01,  1.0056e+00, -9.1582e-01, -1.9572e+00,  5.5210e-02,\n",
       "           2.7064e-01, -1.5950e-01, -9.4920e-01, -3.4615e-01,  7.2554e-01,\n",
       "          -8.1084e-02,  1.1868e+00,  2.0059e+00, -1.6919e-01, -2.6943e-01,\n",
       "          -1.4600e+00,  1.3744e+00,  1.8214e-01, -5.6405e-01, -7.6338e-01,\n",
       "           7.6054e-01, -5.7714e-01, -8.2267e-01,  2.7618e-01,  8.1737e-01,\n",
       "          -7.7062e-01, -1.6287e+00, -1.2557e-01,  2.1198e+00,  5.9238e-01,\n",
       "           8.6997e-01,  1.0667e-02, -7.9374e-01, -4.7307e-01, -9.9546e-01,\n",
       "           1.3974e-01,  2.5155e-01, -8.9208e-01, -9.9399e-01, -1.5619e+00,\n",
       "           1.8046e-03, -4.8847e-01,  1.3973e+00, -8.1856e-01,  2.0498e-01,\n",
       "           1.2718e-01,  5.1120e-01,  6.5488e-01, -1.5950e-01,  1.1275e+00,\n",
       "           5.7560e-01, -7.4113e-01, -6.2638e-01,  1.2195e+00,  2.0684e-01],\n",
       "         [ 1.1513e+00,  1.0539e+00,  3.4105e+00, -9.6206e-01, -1.1720e+00,\n",
       "           5.9532e-01, -4.0978e-01,  1.4256e+00, -1.2171e+00, -1.6845e+00,\n",
       "           5.3848e-01,  1.8967e+00, -2.7450e-01,  2.7868e-01, -6.4734e-01,\n",
       "          -2.6276e+00, -1.3731e+00, -1.2415e+00,  7.0759e-01, -4.9464e-01,\n",
       "           1.1809e+00,  5.4237e-01, -8.5781e-01,  5.1982e-01,  1.5089e-01,\n",
       "          -3.9927e-02,  1.0038e+00, -1.1435e+00,  1.8040e+00, -2.9009e-02,\n",
       "          -8.1313e-01,  9.0933e-01, -1.1375e+00,  5.1402e-01, -4.8947e-01,\n",
       "          -8.0550e-02,  9.1511e-01, -5.4810e-01,  1.1071e+00, -3.5050e-01,\n",
       "           6.6735e-01, -8.9356e-02,  2.7234e-01,  6.0345e-01,  2.3188e-01,\n",
       "           1.5473e+00, -6.8860e-01, -4.4137e-01,  1.2790e+00, -9.9592e-01,\n",
       "          -4.3626e-01, -8.7003e-01, -5.3829e-02,  1.1496e+00,  1.0411e+00,\n",
       "           5.8017e-02, -1.6868e+00,  4.0054e-01,  1.0880e+00, -4.8284e-01,\n",
       "          -7.0947e-02,  1.0966e+00, -5.6861e-01,  9.0792e-01, -1.7011e-01]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9238bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.6382, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)ie(4,8,65)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "\n",
    "        targets = targets.view(B * T)  # (32)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "# idx or xb =(4,8)\n",
    "# returned logits= (4*8,65) stretched vec\n",
    "# losscalculation\n",
    "# The 65-logit vector represents a distribution over choices.\n",
    "# The target integer selects the correct choice, and the loss measures how much probability the model assigned to that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4ba226e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7470, -1.4852,  0.1714,  ...,  2.2019,  2.4498,  0.6347],\n",
      "        [-0.7470, -1.4852,  0.1714,  ...,  2.2019,  2.4498,  0.6347],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        ...,\n",
      "        [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3d91c360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7f12db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50,  1, 61, 43,  1, 51, 39, 56,  1, 51, 53, 56, 43,  8,  0, 28, 59, 56,\n",
      "        42, 43, 56, 43, 56, 10,  0, 32, 46, 47, 57,  1, 54, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T = 4, 8\n",
    "print(yb.view(B * T))\n",
    "yb.view(B * T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629579b",
   "metadata": {},
   "source": [
    "loss calculation complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f645a2a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff1d02",
   "metadata": {},
   "source": [
    "# Generate text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce44b09",
   "metadata": {},
   "source": [
    "1) Part 1  dimension calculation for single step of generation and explanation\n",
    "2) Part 2  implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bd618",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9f5d8",
   "metadata": {},
   "source": [
    "for a single tensor sent to predict next token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12573417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.6382, grad_fn=<NllLossBackward0>)\n",
      "idx begin------\n",
      "idx= tensor([[0]])\n",
      "idxshape torch.Size([1, 1])\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 1, 65])\n",
      "logits_prev= tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
      "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
      "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
      "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
      "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
      "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
      "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
      "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
      "          -0.4032, -0.8345]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
      "          0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
      "          1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
      "          0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n",
      "         -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
      "         -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
      "          1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "          1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n",
      "         -0.8345]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0091, 0.0071, 0.0053, 0.0030, 0.0141, 0.0078, 0.0197, 0.0081, 0.0109,\n",
      "         0.0243, 0.0020, 0.0045, 0.0096, 0.0060, 0.0030, 0.0354, 0.0292, 0.0066,\n",
      "         0.0101, 0.0199, 0.0010, 0.0124, 0.0335, 0.0137, 0.0086, 0.0016, 0.0024,\n",
      "         0.0054, 0.0118, 0.0034, 0.0347, 0.0930, 0.0039, 0.0059, 0.0208, 0.0085,\n",
      "         0.0089, 0.0235, 0.0024, 0.0056, 0.0046, 0.0030, 0.0131, 0.0017, 0.0023,\n",
      "         0.0134, 0.0042, 0.0038, 0.0392, 0.0034, 0.0292, 0.0057, 0.0017, 0.0621,\n",
      "         0.1199, 0.0013, 0.0323, 0.0017, 0.0172, 0.0061, 0.0165, 0.0351, 0.0379,\n",
      "         0.0051, 0.0033]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[31]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "ret_idx= [0, 31]\n",
      "len= 2\n",
      "generated_text \n",
      "S\n"
     ]
    }
   ],
   "source": [
    "# for single next token calculation ,max_new_tokens=1 \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context(1,1)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #   get new prediction\n",
    "            logits, loss = self(idx)\n",
    "            # returns(batch, time, embedding_dim) ie(B,T,C)->(1,1)->(1,1,65)\n",
    "            #during iteration when idx increases egidx=[31,32] logits, loss = self(idx) returns (1,2,65)\n",
    "            #then logits = logits[:, -1, :]  selects last element of timedim so results(1,65)batch,vocab/contextdim\n",
    "            print(\"---\\n\")\n",
    "            print(\"logit_shape_prev\",logits.shape)\n",
    "            print(\"logits_prev=\",logits)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B,C) <-last element in the time dim,,,just one time dim so selects that whole tensor(1,1)->(1,1,65)->(1,65)\n",
    "            # applying softmax to get probabilities form logits\n",
    "            print(\"---\\n\")\n",
    "            print(\"logits_next=\",logits)\n",
    "            print(\"logit_shape_next\",logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            print(\"---\\n\")\n",
    "            print(\"probs=\",probs)\n",
    "            print(\"probsshape\",probs.shape)#(1,65)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)ie(1,1)selects any one token from the probability values from 65 of them\n",
    "            #   append sampled index to the running sequence\n",
    "            # Selects the next token based on the probability of each token, so higher-probability tokens are more likely but not guaranteed.\n",
    "            print(\"---\\n\")\n",
    "            print(\"idx_next=\",idx_next)\n",
    "            print(\"idx_nextshape\",idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            #eg idx=[31,32]\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# PyTorch expects a batch dimension in tensors, so even a single sequence must be shaped as (B, T) rather than just (T).\n",
    "print(\"idx begin------\")\n",
    "print(\"idx=\",idx)\n",
    "print(\"idxshape\",idx.shape)\n",
    "ret_idx=m.generate(idx, max_new_tokens=1)[0].tolist()\n",
    "print(\"ret_idx=\",ret_idx)\n",
    "print(\"len=\",len(ret_idx))\n",
    "print(\"generated_text\",decode_txt(ret_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b5c18d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generatedtext must be \\nS. char 1 by one \\n is treated ans new line here in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "2e21f867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0), (' ', 1), ('!', 2), ('$', 3), ('&', 4)]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[:5]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6817a2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "bf7ab1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([torch.zeros((1), dtype=torch.long).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7fc6bedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c9b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.6382, grad_fn=<NllLossBackward0>)\n",
      "idx= tensor([[31]])\n",
      "idxshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  1\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 1, 65])\n",
      "logits_prev= tensor([[[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
      "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
      "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
      "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
      "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
      "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
      "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
      "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "           0.2930,  0.0943]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104, -1.0886,\n",
      "          1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,  1.6030, -0.0726,\n",
      "         -0.4725, -1.1616,  0.5962,  1.3058, -0.7422, -1.2529,  0.6750,  1.5664,\n",
      "         -0.9238, -0.0956, -1.5452, -0.1801,  3.1838, -0.1277,  0.0910,  0.5422,\n",
      "         -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,\n",
      "          1.2588, -0.0644, -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,\n",
      "          1.5294, -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "          0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,\n",
      "          0.0943]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0027, 0.0042, 0.0052, 0.0105, 0.0153, 0.0028, 0.0028, 0.0026, 0.0292,\n",
      "         0.0140, 0.0026, 0.0021, 0.0034, 0.0204, 0.0384, 0.0072, 0.0048, 0.0024,\n",
      "         0.0140, 0.0285, 0.0037, 0.0022, 0.0152, 0.0370, 0.0031, 0.0070, 0.0016,\n",
      "         0.0065, 0.1867, 0.0068, 0.0085, 0.0133, 0.0042, 0.0130, 0.0655, 0.0019,\n",
      "         0.0033, 0.0213, 0.0148, 0.0099, 0.0272, 0.0073, 0.0029, 0.0047, 0.0060,\n",
      "         0.0051, 0.0036, 0.0231, 0.0357, 0.0022, 0.0222, 0.0129, 0.0114, 0.0032,\n",
      "         0.0297, 0.0818, 0.0086, 0.0022, 0.0171, 0.0036, 0.0067, 0.0042, 0.0212,\n",
      "         0.0104, 0.0085]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[38]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  2\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 2, 65])\n",
      "logits_prev= tensor([[[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
      "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
      "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
      "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
      "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
      "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
      "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
      "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "           0.2930,  0.0943],\n",
      "         [-1.6774,  0.7494,  1.4538, -2.2956, -1.6489, -0.9055, -0.4162,\n",
      "          -0.0636, -0.6666,  0.3252, -0.1097,  1.2385,  0.5742,  1.4636,\n",
      "           1.1599, -1.5200, -1.3161, -0.9729, -1.0606,  0.6451, -1.9633,\n",
      "          -0.0105,  0.8958,  0.6347, -0.2572,  0.0489, -0.3030,  0.9117,\n",
      "           0.2386, -1.5984,  1.4978,  0.3207,  0.4126, -0.5031,  0.8067,\n",
      "          -0.2202,  0.8364,  1.3637, -0.6479,  0.8207,  0.4370,  0.0943,\n",
      "           0.8609, -0.2474, -2.3996,  0.3896, -0.4067, -0.9901, -0.7057,\n",
      "          -1.5055, -0.6710, -0.9790, -0.9228, -0.5879,  0.3140, -0.3093,\n",
      "           0.0045, -0.8232, -0.4622, -1.3261,  1.1315, -0.5476,  1.1353,\n",
      "           0.7542, -1.1444]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[-1.6774,  0.7494,  1.4538, -2.2956, -1.6489, -0.9055, -0.4162, -0.0636,\n",
      "         -0.6666,  0.3252, -0.1097,  1.2385,  0.5742,  1.4636,  1.1599, -1.5200,\n",
      "         -1.3161, -0.9729, -1.0606,  0.6451, -1.9633, -0.0105,  0.8958,  0.6347,\n",
      "         -0.2572,  0.0489, -0.3030,  0.9117,  0.2386, -1.5984,  1.4978,  0.3207,\n",
      "          0.4126, -0.5031,  0.8067, -0.2202,  0.8364,  1.3637, -0.6479,  0.8207,\n",
      "          0.4370,  0.0943,  0.8609, -0.2474, -2.3996,  0.3896, -0.4067, -0.9901,\n",
      "         -0.7057, -1.5055, -0.6710, -0.9790, -0.9228, -0.5879,  0.3140, -0.3093,\n",
      "          0.0045, -0.8232, -0.4622, -1.3261,  1.1315, -0.5476,  1.1353,  0.7542,\n",
      "         -1.1444]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0022, 0.0251, 0.0508, 0.0012, 0.0023, 0.0048, 0.0078, 0.0111, 0.0061,\n",
      "         0.0164, 0.0106, 0.0410, 0.0211, 0.0513, 0.0379, 0.0026, 0.0032, 0.0045,\n",
      "         0.0041, 0.0226, 0.0017, 0.0118, 0.0291, 0.0224, 0.0092, 0.0125, 0.0088,\n",
      "         0.0296, 0.0151, 0.0024, 0.0531, 0.0164, 0.0180, 0.0072, 0.0266, 0.0095,\n",
      "         0.0274, 0.0465, 0.0062, 0.0270, 0.0184, 0.0131, 0.0281, 0.0093, 0.0011,\n",
      "         0.0175, 0.0079, 0.0044, 0.0059, 0.0026, 0.0061, 0.0045, 0.0047, 0.0066,\n",
      "         0.0163, 0.0087, 0.0119, 0.0052, 0.0075, 0.0032, 0.0368, 0.0069, 0.0370,\n",
      "         0.0253, 0.0038]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[2]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  3\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 3, 65])\n",
      "logits_prev= tensor([[[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
      "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
      "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
      "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
      "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
      "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
      "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
      "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "           0.2930,  0.0943],\n",
      "         [-1.6774,  0.7494,  1.4538, -2.2956, -1.6489, -0.9055, -0.4162,\n",
      "          -0.0636, -0.6666,  0.3252, -0.1097,  1.2385,  0.5742,  1.4636,\n",
      "           1.1599, -1.5200, -1.3161, -0.9729, -1.0606,  0.6451, -1.9633,\n",
      "          -0.0105,  0.8958,  0.6347, -0.2572,  0.0489, -0.3030,  0.9117,\n",
      "           0.2386, -1.5984,  1.4978,  0.3207,  0.4126, -0.5031,  0.8067,\n",
      "          -0.2202,  0.8364,  1.3637, -0.6479,  0.8207,  0.4370,  0.0943,\n",
      "           0.8609, -0.2474, -2.3996,  0.3896, -0.4067, -0.9901, -0.7057,\n",
      "          -1.5055, -0.6710, -0.9790, -0.9228, -0.5879,  0.3140, -0.3093,\n",
      "           0.0045, -0.8232, -0.4622, -1.3261,  1.1315, -0.5476,  1.1353,\n",
      "           0.7542, -1.1444],\n",
      "         [ 1.3035, -0.4501,  1.3471,  1.6910, -0.1244, -1.6824, -0.0266,\n",
      "           0.0740,  1.0517,  0.6779,  0.3067, -0.7472,  0.7435,  0.8877,\n",
      "           2.2874,  0.9611, -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,\n",
      "           1.1385, -0.9913,  0.1700,  1.2249, -0.2345, -1.0572, -0.6543,\n",
      "           1.5909, -0.6995, -0.8961,  0.0662, -0.0563,  2.3412, -2.7234,\n",
      "           0.5097, -0.8145, -0.2460,  0.0045,  2.0474, -0.1575, -0.2187,\n",
      "          -1.3519, -0.0573, -1.8540, -1.3849, -0.3454, -1.1625,  0.1445,\n",
      "           0.1663,  0.7507,  0.9132, -1.7277,  1.3055,  0.9593,  1.0600,\n",
      "           0.6299, -1.2867, -0.6875,  2.1382,  0.5114,  1.2191,  0.1910,\n",
      "          -0.3425,  1.7955]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 1.3035, -0.4501,  1.3471,  1.6910, -0.1244, -1.6824, -0.0266,  0.0740,\n",
      "          1.0517,  0.6779,  0.3067, -0.7472,  0.7435,  0.8877,  2.2874,  0.9611,\n",
      "         -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,  1.1385, -0.9913,  0.1700,\n",
      "          1.2249, -0.2345, -1.0572, -0.6543,  1.5909, -0.6995, -0.8961,  0.0662,\n",
      "         -0.0563,  2.3412, -2.7234,  0.5097, -0.8145, -0.2460,  0.0045,  2.0474,\n",
      "         -0.1575, -0.2187, -1.3519, -0.0573, -1.8540, -1.3849, -0.3454, -1.1625,\n",
      "          0.1445,  0.1663,  0.7507,  0.9132, -1.7277,  1.3055,  0.9593,  1.0600,\n",
      "          0.6299, -1.2867, -0.6875,  2.1382,  0.5114,  1.2191,  0.1910, -0.3425,\n",
      "          1.7955]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0288, 0.0050, 0.0301, 0.0424, 0.0069, 0.0015, 0.0076, 0.0084, 0.0224,\n",
      "         0.0154, 0.0106, 0.0037, 0.0164, 0.0190, 0.0770, 0.0204, 0.0017, 0.0058,\n",
      "         0.0070, 0.0057, 0.0042, 0.0244, 0.0029, 0.0093, 0.0266, 0.0062, 0.0027,\n",
      "         0.0041, 0.0384, 0.0039, 0.0032, 0.0084, 0.0074, 0.0813, 0.0005, 0.0130,\n",
      "         0.0035, 0.0061, 0.0079, 0.0606, 0.0067, 0.0063, 0.0020, 0.0074, 0.0012,\n",
      "         0.0020, 0.0055, 0.0024, 0.0090, 0.0092, 0.0166, 0.0195, 0.0014, 0.0288,\n",
      "         0.0204, 0.0226, 0.0147, 0.0022, 0.0039, 0.0663, 0.0130, 0.0265, 0.0095,\n",
      "         0.0056, 0.0471]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[21]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "ret_idx= [31, 38, 2, 21]\n",
      "len= 4\n",
      "generated_text SZ!I\n"
     ]
    }
   ],
   "source": [
    "# for max_new_tokens=3\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            print(\"******\"*8,\"\\ntoken no= \",_+1)\n",
    "            #   get new predication\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            print(\"---\\n\")\n",
    "            print(\"logit_shape_prev\",logits.shape)\n",
    "            print(\"logits_prev=\",logits)\n",
    "            logits = logits[:, -1, :]  # becomes (B,C) <-last element in the time dim\n",
    "            # applying softmax to get probabilities form logits\n",
    "            print(\"---\\n\")\n",
    "            print(\"logits_next=\",logits)\n",
    "            print(\"logit_shape_next\",logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            print(\"---\\n\")\n",
    "            print(\"probs=\",probs)\n",
    "            print(\"probsshape\",probs.shape)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "            #   append sampled index to the running sequence\n",
    "            print(\"---\\n\")\n",
    "            print(\"idx_next=\",idx_next)\n",
    "            print(\"idx_nextshape\",idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.tensor([[31]],dtype=torch.long)\n",
    "print(\"idx=\",idx)\n",
    "print(\"idxshape\",idx.shape)\n",
    "ret_idx=m.generate((idx), max_new_tokens=3)[0].tolist()\n",
    "print(\"ret_idx=\",ret_idx)\n",
    "print(\"len=\",len(ret_idx))\n",
    "print(\"generated_text\",decode_txt(ret_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "7108a5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[31]],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "9096abad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97df0b",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b3a24",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "efb34161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.6382, grad_fn=<NllLossBackward0>)\n",
      "idx begin------\n",
      "idx= tensor([[0]])\n",
      "idxshape torch.Size([1, 1])\n",
      "ret_idx= [0, 31, 23, 21, 41, 24, 32, 11, 13, 41, 17, 24, 25, 53, 32, 40, 60, 38, 60, 1, 15, 12, 52, 55, 7, 29, 17, 9, 9, 10, 15, 22, 55, 49, 27, 23, 20, 7, 55, 11, 10, 50, 39, 2, 53, 47, 63, 61, 49, 20, 48, 45, 15, 46, 64, 40, 29, 12, 59, 2, 9, 40, 24, 21, 45, 61, 43, 60, 51, 63, 18, 22, 19, 33, 19, 54, 0, 61, 52, 37, 35, 51, 52, 62, 23, 35, 35, 43, 60, 7, 58, 16, 55, 36, 17, 56, 34, 23, 24, 45, 22]\n",
      "len= 101\n",
      "generated_text \n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context(1,1)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #   get new prediction\n",
    "            logits, loss = self(idx)\n",
    "            # returns(batch, time, embedding_dim) ie(B,T,C)->(1,1)->(1,1,65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B,C) <-last element in the time dim,,,just one time dim so selects that whole tensor(1,1)->(1,1,65)->(1,65)\n",
    "            # applying softmax to get probabilities form logits\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)ie(1,1)selects any one token from the probability values from 65 of them\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            #eg next = idx=[31,32]\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# 0 index in vocab represents \\n\n",
    "# PyTorch expects a batch dimension in tensors, so even a single sequence must be shaped as (B, T) rather than just (T).\n",
    "print(\"idx begin------\")\n",
    "print(\"idx=\",idx)\n",
    "print(\"idxshape\",idx.shape)\n",
    "ret_idx=m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(\"ret_idx=\",ret_idx)\n",
    "print(\"len=\",len(ret_idx))\n",
    "print(\"generated_text\",decode_txt(ret_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52eb39",
   "metadata": {},
   "source": [
    "result is sort of garbage because next token is predicted only on the basis of current token without any context of previous tokens.\n",
    "for prediction of T SKIcLT  model only sees L and predicts next token T ,without any context of previous tokens SKIc.\n",
    "- next task is to make model see previous tokens as context not just the current token while predicting next token.\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4d8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_ds_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
