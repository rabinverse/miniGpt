{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e06db4",
   "metadata": {},
   "source": [
    "<img src=\"../dataset_research_paper_docs/transformer_archi.png\" alt=\"transformer\" width=\"600\" style=\"display:block; margin:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86778a65",
   "metadata": {},
   "source": [
    "# Char level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095db2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x138b49370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ba565484",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset_research_paper_docs/input_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1a38df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "56d2ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "257b9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "{'p', 'P', 'J', 'w', 'v', \"'\", '.', 'Y', 'l', ' ', 't', 'A', 'O', 'r', 'u', 'h', 'D', 'f', 'k', 'i', '-', 'q', '!', 'a', 's', 'X', 'B', 'o', 'n', '$', 'Z', 'z', 'b', 'F', 'I', 'H', 'x', '\\n', 'c', 'y', 'V', 'N', 'G', 'm', 'K', 'g', 'j', 'W', 'E', ':', 'R', 'U', 'S', 'Q', '&', '3', ';', ',', 'L', 'M', '?', 'd', 'e', 'T', 'C'}\n"
     ]
    }
   ],
   "source": [
    "chars = set(text)\n",
    "print(len(chars))\n",
    "print(chars, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "65def49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'P', 'J', 'w', 'v', \"'\", '.', 'Y', 'l', ' ']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145637e9",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "40e7ef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10679d",
   "metadata": {},
   "source": [
    "## `create` a mapping table for string to integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4a83b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strtoint = {ch: i for i, ch in enumerate(chars)}\n",
    "inttostr = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode_txt = lambda s: [strtoint[c] for c in s]\n",
    "# returns list of integer for input string given\n",
    "\n",
    "decode_txt = lambda l: \"\".join(inttostr[i] for i in l)\n",
    "# returns string from given integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d64b93f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0),\n",
       " (' ', 1),\n",
       " ('!', 2),\n",
       " ('$', 3),\n",
       " ('&', 4),\n",
       " (\"'\", 5),\n",
       " (',', 6),\n",
       " ('-', 7),\n",
       " ('.', 8),\n",
       " ('3', 9)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[:10]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8e6280d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('q', 55),\n",
       " ('r', 56),\n",
       " ('s', 57),\n",
       " ('t', 58),\n",
       " ('u', 59),\n",
       " ('v', 60),\n",
       " ('w', 61),\n",
       " ('x', 62),\n",
       " ('y', 63),\n",
       " ('z', 64)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[-10:]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "09b2be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(55, 'q'),\n",
       " (56, 'r'),\n",
       " (57, 's'),\n",
       " (58, 't'),\n",
       " (59, 'u'),\n",
       " (60, 'v'),\n",
       " (61, 'w'),\n",
       " (62, 'x'),\n",
       " (63, 'y'),\n",
       " (64, 'z')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inttostr.items())[-10:]  # lookuptable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eaaaf8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30417",
   "metadata": {},
   "source": [
    "Character level token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c7bdd908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 46, 39, 58, 1, 64, 62, 63, 1, 51, 53, 53, 59, 52, 58, 39, 47, 52, 1]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_txt(\"what zxy moountain \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f452d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what zxy moountain '"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([61, 46, 39, 58, 1, 64, 62, 63, 1, 51, 53, 53, 59, 52, 58, 39, 47, 52, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fc598895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 54, 43, 53, 54, 50, 43]\n",
      "hello people\n"
     ]
    }
   ],
   "source": [
    "print(encode_txt(\"hello people\"))\n",
    "\n",
    "enc_text = encode_txt(\"hello people\")\n",
    "\n",
    "print(decode_txt(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590ad12",
   "metadata": {},
   "source": [
    "# Google uses [sentencepiece](https://github.com/google/sentencepiece) for tokenization.\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e553b",
   "metadata": {},
   "source": [
    "# OpenAI uses Byte Pair Encoding [BPE](https://github.com/openai/tiktoken) for tokenization.\n",
    "\n",
    "BPE is a simple form of data compression that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. In the context of tokenization, BPE is used to create a vocabulary of subword units that can efficiently represent text data. The algorithm starts with a base vocabulary of individual characters and then merges the most frequent pairs of characters or subwords to form new tokens. This process continues until a predefined vocabulary size is reached. BPE is particularly effective for handling out-of-vocabulary words and capturing common patterns in text, making it a popular choice for tokenization in natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0635cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4f1451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7d8576c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8a4be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 2506]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi everyone'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(enc.encode(\"Hi everyone\"))\n",
    "that = enc.encode(\"Hi everyone\")\n",
    "enc.decode(that)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406dd94",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46892bc4",
   "metadata": {},
   "source": [
    "`Encode` the whole shakespeare text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "00349e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "af37bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "# encode whole text\n",
    "data = torch.tensor(encode_txt(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "# print first 500 character encoding\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81db651",
   "metadata": {},
   "source": [
    "# `split` the data to train test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "db1149ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "94fdfda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003853\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "print(n)\n",
    "\n",
    "\n",
    "# first 90% in the train and rest 10% in the val\n",
    "\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e67a9",
   "metadata": {},
   "source": [
    "while training we dont give the model the full sequence rather we give part of the sequence and do it in batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12c6ca",
   "metadata": {},
   "source": [
    "block size or context length : how many tokens the model can see at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1f32549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2d6cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given -> tensor([18, 47, 56, 57, 58,  1, 15, 47]) predict -> tensor(58) total -> tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"given ->\",\n",
    "    train_data[:block_size],\n",
    "    \"predict ->\",\n",
    "    train_data[block_size],\n",
    "    \"total ->\",\n",
    "    train_data[: block_size + 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "47de7c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([18]) o/p --->  tensor(47)\n",
      "when input is  tensor([18, 47]) o/p --->  tensor(56)\n",
      "when input is  tensor([18, 47, 56]) o/p --->  tensor(57)\n",
      "when input is  tensor([18, 47, 56, 57]) o/p --->  tensor(58)\n",
      "when input is  tensor([18, 47, 56, 57, 58]) o/p --->  tensor(1)\n",
      "when input is  tensor([18, 47, 56, 57, 58,  1]) o/p --->  tensor(15)\n",
      "when input is  tensor([18, 47, 56, 57, 58,  1, 15]) o/p --->  tensor(47)\n",
      "when input is  tensor([18, 47, 56, 57, 58,  1, 15, 47]) o/p --->  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# x is the input to the transformer --first block size characters\n",
    "# y is offset by 1 to x ----- next block size character. - y is the target for each position to the input\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(\"when input is \", context, \"o/p ---> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26999319",
   "metadata": {},
   "source": [
    "there is a new dimension batch dimension\n",
    "while training we dont give the model the full sequence rather we give part of the sequence and do it in batches.\n",
    "\n",
    "batches of sequences of block size length are fed for efficiency to process in parallel\n",
    "\n",
    "batch of sequence of block size length are stacked in tensor and fed to process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "047546ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # length of the input sequence\n",
    "batch_size = 4  # no of input sequence to process in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "65cee5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115385"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) - block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c5b45716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003845"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) - block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "84c5f818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([279418, 386416,  68800, 755839])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# four independent rows\n",
    "\n",
    "\n",
    "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "331924ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(279418)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(52)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ix[0])\n",
    "data[ix[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efc030",
   "metadata": {},
   "source": [
    "in a batch,completely independent sequences are selected randomly of block size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2b19b8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[52, 57,  1, 57, 51, 53, 58, 46],\n",
      "        [ 1, 44, 47, 45, 46, 58, 10,  0],\n",
      "        [58,  1, 52, 53, 58,  1, 39,  1],\n",
      "        [ 8,  0, 32, 46, 39, 58,  1, 21]])\n",
      "tensor([[57,  1, 57, 51, 53, 58, 46, 43],\n",
      "        [44, 47, 45, 46, 58, 10,  0, 13],\n",
      "        [ 1, 52, 53, 58,  1, 39,  1, 54],\n",
      "        [ 0, 32, 46, 39, 58,  1, 21,  1]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "89103742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdac65",
   "metadata": {},
   "source": [
    "- when generating block size of context length(block size continuous adjacent data) ,subtract len(data)-block size to avoid out of index\n",
    "- ie block size needed is 8 and if data length is 100 and block size is 8 ,the max starting point can be 92 to get 92 to 100 (8 length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e727877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8])\n",
      "tensor([[54, 43, 39, 49,  1, 39, 45, 39],\n",
      "        [43, 56, 57, 11,  1, 61, 46, 53],\n",
      "        [43,  1, 46, 39, 58, 46,  1, 58],\n",
      "        [43,  1, 47, 57,  1, 57, 53,  1]])\n",
      "----\n",
      " \n",
      "targets:  torch.Size([4, 8])\n",
      "tensor([[43, 39, 49,  1, 39, 45, 39, 47],\n",
      "        [56, 57, 11,  1, 61, 46, 53,  6],\n",
      "        [ 1, 46, 39, 58, 46,  1, 58, 56],\n",
      "        [ 1, 47, 57,  1, 57, 53,  1, 50]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8  # length of the input sequence\n",
    "batch_size = 4  # no of input sequence to process in parallel\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs: \", xb.shape)\n",
    "print(xb)\n",
    "print(\"----\\n \")\n",
    "print(\"targets: \", yb.shape)\n",
    "print(yb)\n",
    "\n",
    "# xb is the input to the transformer\n",
    "\n",
    "# for 54 target is 43,for 43 target is 39 for single token target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ab6bbecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8\n"
     ]
    }
   ],
   "source": [
    "print(batch_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1a17f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  [54] output -->  tensor(43)\n",
      "when input is  [54, 43] output -->  tensor(39)\n",
      "when input is  [54, 43, 39] output -->  tensor(49)\n",
      "when input is  [54, 43, 39, 49] output -->  tensor(1)\n",
      "when input is  [54, 43, 39, 49, 1] output -->  tensor(39)\n",
      "when input is  [54, 43, 39, 49, 1, 39] output -->  tensor(45)\n",
      "when input is  [54, 43, 39, 49, 1, 39, 45] output -->  tensor(39)\n",
      "when input is  [54, 43, 39, 49, 1, 39, 45, 39] output -->  tensor(47)\n",
      "when input is  [43] output -->  tensor(56)\n",
      "when input is  [43, 56] output -->  tensor(57)\n",
      "when input is  [43, 56, 57] output -->  tensor(11)\n",
      "when input is  [43, 56, 57, 11] output -->  tensor(1)\n",
      "when input is  [43, 56, 57, 11, 1] output -->  tensor(61)\n",
      "when input is  [43, 56, 57, 11, 1, 61] output -->  tensor(46)\n",
      "when input is  [43, 56, 57, 11, 1, 61, 46] output -->  tensor(53)\n",
      "when input is  [43, 56, 57, 11, 1, 61, 46, 53] output -->  tensor(6)\n",
      "when input is  [43] output -->  tensor(1)\n",
      "when input is  [43, 1] output -->  tensor(46)\n",
      "when input is  [43, 1, 46] output -->  tensor(39)\n",
      "when input is  [43, 1, 46, 39] output -->  tensor(58)\n",
      "when input is  [43, 1, 46, 39, 58] output -->  tensor(46)\n",
      "when input is  [43, 1, 46, 39, 58, 46] output -->  tensor(1)\n",
      "when input is  [43, 1, 46, 39, 58, 46, 1] output -->  tensor(58)\n",
      "when input is  [43, 1, 46, 39, 58, 46, 1, 58] output -->  tensor(56)\n",
      "when input is  [43] output -->  tensor(1)\n",
      "when input is  [43, 1] output -->  tensor(47)\n",
      "when input is  [43, 1, 47] output -->  tensor(57)\n",
      "when input is  [43, 1, 47, 57] output -->  tensor(1)\n",
      "when input is  [43, 1, 47, 57, 1] output -->  tensor(57)\n",
      "when input is  [43, 1, 47, 57, 1, 57] output -->  tensor(53)\n",
      "when input is  [43, 1, 47, 57, 1, 57, 53] output -->  tensor(1)\n",
      "when input is  [43, 1, 47, 57, 1, 57, 53, 1] output -->  tensor(50)\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    for j in range(block_size):\n",
    "        context = xb[i, : j + 1]\n",
    "        target = yb[i, j]\n",
    "        print(\"when input is \", context.tolist(), \"output --> \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b1c2",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b639ae",
   "metadata": {},
   "source": [
    "## start feeding to NN\n",
    "\n",
    "- Bigram model\n",
    "  - simple model for language modeling that predicts the next token based on the current token using a lookup table.\n",
    "  - each token in the vocabulary has a corresponding embedding vector in the lookup table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d8e1d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of size (vocab_size, vocab_size) where each row corresponds to a token in the vocabulary and contains the logits for predicting the next token.\n",
    "\n",
    "# here each token is made to (65\\*65)\n",
    "\n",
    "# Embedding =  matrix of shape (num_embeddings, embedding_dim)\n",
    "\n",
    "# when\n",
    "# logits = self.token_embedding_table(idx)\n",
    "# internally\n",
    "# logits[b, t] = W[idx[b, t]]\n",
    "\n",
    "# The embedding table is formed by initializing a (vocab_size Ã— vocab_size) matrix with random values and then gradually shaping each row through gradient descent so that it learns the logits for predicting the next token given the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3e1996f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape)\n",
    "\n",
    "# idx or xb =(4,8)\n",
    "# returned logits= (4,8,65)(4batch of 8dim with 65vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "44dd6bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "76f80b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 44, 47, 45, 46, 58, 10,  0])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ff2ed",
   "metadata": {},
   "source": [
    "#returned logits= (4,8,65)(4batch of 65dim vector for each of the 8 tokens in the sequence)\n",
    "\n",
    "- Each integer in the 8-length vector becomes a 65-length vector\n",
    "- for x[4,8] 4 vec of 8dimlength each logits returns as (4,8,65) ,4batch of 65dim vector for each of the 8 tokens in the sequence\n",
    "- [ 0, 32, 46, 53, 59, 1, 40, 43] each integer in 8-length vector becomes a 65-length vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f2afe94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 65])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5cd48a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.7874e-01,  8.6619e-01, -1.6433e+00,  1.9448e+00, -1.6309e-01,\n",
       "          -1.7145e-01,  9.3094e-01, -8.5501e-01,  7.5243e-01,  2.6255e-01,\n",
       "          -1.3360e+00, -9.7573e-01, -5.1340e-01,  7.1156e-01, -1.9585e-02,\n",
       "           4.3529e-01,  8.2789e-01,  5.4866e-01, -4.3626e-01,  5.5188e-01,\n",
       "           6.7815e-01,  2.5347e-01,  1.6661e+00, -6.9642e-01, -3.1699e-01,\n",
       "           8.4865e-01,  4.3408e-01, -2.2396e-01, -1.0157e+00, -1.1612e-01,\n",
       "           2.8764e-01,  4.2026e-01,  3.3789e-01,  8.0157e-01, -1.7313e-01,\n",
       "           5.8086e-01,  1.6225e-01,  1.3970e+00,  3.3073e-01,  2.7711e-01,\n",
       "           7.4096e-01,  2.7270e-01,  2.3462e-01, -1.1172e-01, -5.6908e-01,\n",
       "           1.1829e+00,  1.9441e+00, -4.1552e-01, -3.6204e-01, -1.7652e-01,\n",
       "          -9.3482e-01,  1.5461e+00, -8.5770e-01, -1.0060e-01,  3.5597e-01,\n",
       "          -1.6589e+00,  6.5434e-01, -1.3299e+00,  1.1929e+00,  4.8549e-01,\n",
       "          -5.7211e-01,  1.0813e+00,  2.3671e+00, -7.7751e-01, -2.5861e-01],\n",
       "         [ 3.3227e-01, -8.7153e-02, -7.4698e-01, -6.0736e-01,  3.4183e-01,\n",
       "           5.3435e-01,  3.9569e-01, -4.9194e-01, -8.9385e-02, -1.3886e+00,\n",
       "           1.2835e+00, -3.9750e-01,  2.0152e+00,  1.6773e+00, -3.8328e-01,\n",
       "           1.5728e+00,  1.9458e+00,  7.2471e-01, -4.8339e-01, -3.2629e-01,\n",
       "           3.1928e-01, -4.1984e-01, -6.4349e-01, -3.3106e-01,  7.5537e-01,\n",
       "          -1.2385e+00,  4.0670e-01,  9.9823e-01, -6.5108e-01,  1.2450e+00,\n",
       "           2.8036e-01,  8.3712e-01, -4.1192e-01,  2.1150e-01, -6.2398e-01,\n",
       "           2.0280e-02, -3.4183e-01,  1.4934e+00,  1.7307e+00,  1.3354e+00,\n",
       "          -2.7121e-01,  4.9022e-01,  6.6004e-01, -1.6321e+00, -7.8585e-01,\n",
       "           1.7688e+00,  2.6160e+00, -5.7669e-01, -3.6284e-01, -2.7428e+00,\n",
       "           7.4275e-01,  7.3699e-02,  2.0505e-01, -5.4975e-01,  2.1261e+00,\n",
       "          -9.2399e-01,  1.0475e-01,  8.3239e-01,  1.4287e+00, -7.7891e-01,\n",
       "           2.9275e+00, -8.5249e-01, -6.7158e-01, -9.5724e-01, -9.5944e-01],\n",
       "         [ 1.1513e+00,  1.0539e+00,  3.4105e+00, -9.6206e-01, -1.1720e+00,\n",
       "           5.9532e-01, -4.0978e-01,  1.4256e+00, -1.2171e+00, -1.6845e+00,\n",
       "           5.3848e-01,  1.8967e+00, -2.7450e-01,  2.7868e-01, -6.4734e-01,\n",
       "          -2.6276e+00, -1.3731e+00, -1.2415e+00,  7.0759e-01, -4.9464e-01,\n",
       "           1.1809e+00,  5.4237e-01, -8.5781e-01,  5.1982e-01,  1.5089e-01,\n",
       "          -3.9927e-02,  1.0038e+00, -1.1435e+00,  1.8040e+00, -2.9009e-02,\n",
       "          -8.1313e-01,  9.0933e-01, -1.1375e+00,  5.1402e-01, -4.8947e-01,\n",
       "          -8.0550e-02,  9.1511e-01, -5.4810e-01,  1.1071e+00, -3.5050e-01,\n",
       "           6.6735e-01, -8.9356e-02,  2.7234e-01,  6.0345e-01,  2.3188e-01,\n",
       "           1.5473e+00, -6.8860e-01, -4.4137e-01,  1.2790e+00, -9.9592e-01,\n",
       "          -4.3626e-01, -8.7003e-01, -5.3829e-02,  1.1496e+00,  1.0411e+00,\n",
       "           5.8017e-02, -1.6868e+00,  4.0054e-01,  1.0880e+00, -4.8284e-01,\n",
       "          -7.0947e-02,  1.0966e+00, -5.6861e-01,  9.0792e-01, -1.7011e-01],\n",
       "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
       "          -1.6497e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
       "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
       "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
       "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
       "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
       "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
       "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
       "           6.8222e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
       "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
       "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
       "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
       "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00],\n",
       "         [ 5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,  4.6576e-01,\n",
       "          -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,  2.2280e-01,\n",
       "           6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,  2.2800e-01,\n",
       "           2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01, -1.5935e+00,\n",
       "          -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01, -3.4189e-01,\n",
       "           4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01, -1.0012e+00,\n",
       "          -4.0943e-01, -1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,\n",
       "           3.1549e-02, -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01,\n",
       "          -1.3343e-01,  2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00,\n",
       "          -6.4046e-01, -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01,\n",
       "          -1.8846e+00,  1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,\n",
       "           1.7510e+00,  2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,\n",
       "           3.1845e-01, -5.4959e-01, -1.4649e+00, -2.0555e+00,  1.8275e+00],\n",
       "         [ 1.1513e+00,  1.0539e+00,  3.4105e+00, -9.6206e-01, -1.1720e+00,\n",
       "           5.9532e-01, -4.0978e-01,  1.4256e+00, -1.2171e+00, -1.6845e+00,\n",
       "           5.3848e-01,  1.8967e+00, -2.7450e-01,  2.7868e-01, -6.4734e-01,\n",
       "          -2.6276e+00, -1.3731e+00, -1.2415e+00,  7.0759e-01, -4.9464e-01,\n",
       "           1.1809e+00,  5.4237e-01, -8.5781e-01,  5.1982e-01,  1.5089e-01,\n",
       "          -3.9927e-02,  1.0038e+00, -1.1435e+00,  1.8040e+00, -2.9009e-02,\n",
       "          -8.1313e-01,  9.0933e-01, -1.1375e+00,  5.1402e-01, -4.8947e-01,\n",
       "          -8.0550e-02,  9.1511e-01, -5.4810e-01,  1.1071e+00, -3.5050e-01,\n",
       "           6.6735e-01, -8.9356e-02,  2.7234e-01,  6.0345e-01,  2.3188e-01,\n",
       "           1.5473e+00, -6.8860e-01, -4.4137e-01,  1.2790e+00, -9.9592e-01,\n",
       "          -4.3626e-01, -8.7003e-01, -5.3829e-02,  1.1496e+00,  1.0411e+00,\n",
       "           5.8017e-02, -1.6868e+00,  4.0054e-01,  1.0880e+00, -4.8284e-01,\n",
       "          -7.0947e-02,  1.0966e+00, -5.6861e-01,  9.0792e-01, -1.7011e-01],\n",
       "         [ 6.6348e-01,  2.6726e-01, -4.0968e-02, -4.1045e-01, -7.9258e-01,\n",
       "          -4.5052e-01, -1.2630e+00, -1.1049e-01, -1.5258e+00, -2.4088e+00,\n",
       "           6.2567e-01, -7.8628e-01, -1.3341e-01, -5.0673e-01,  4.9312e-01,\n",
       "           3.1957e+00, -6.9719e-01,  1.4158e-01,  1.1991e+00,  8.4574e-01,\n",
       "           1.1119e+00,  7.5411e-01,  5.6716e-01,  1.0343e+00,  4.2398e-01,\n",
       "          -3.9114e-01, -2.2213e+00,  1.7533e+00, -1.2363e+00,  1.1138e+00,\n",
       "           1.7952e+00,  2.8732e-01, -1.9710e-01,  1.2285e+00,  9.5278e-03,\n",
       "           2.2227e-01,  1.9963e+00,  1.3765e+00,  1.0229e+00, -1.3247e-03,\n",
       "           7.7858e-01, -2.9392e-01,  1.2563e+00, -7.8401e-01,  8.0610e-01,\n",
       "          -3.7246e-01, -8.1083e-01,  8.6826e-01,  7.9161e-01,  6.6330e-01,\n",
       "           1.8970e-01,  1.7075e+00,  7.7272e-01, -2.6976e-01, -7.1044e-01,\n",
       "           1.7779e+00, -7.2955e-01, -8.2731e-01, -2.5742e+00, -3.9104e-01,\n",
       "           2.3160e-02,  8.5039e-01, -5.8610e-01, -1.0893e+00,  1.9482e-01],\n",
       "         [ 1.1513e+00,  1.0539e+00,  3.4105e+00, -9.6206e-01, -1.1720e+00,\n",
       "           5.9532e-01, -4.0978e-01,  1.4256e+00, -1.2171e+00, -1.6845e+00,\n",
       "           5.3848e-01,  1.8967e+00, -2.7450e-01,  2.7868e-01, -6.4734e-01,\n",
       "          -2.6276e+00, -1.3731e+00, -1.2415e+00,  7.0759e-01, -4.9464e-01,\n",
       "           1.1809e+00,  5.4237e-01, -8.5781e-01,  5.1982e-01,  1.5089e-01,\n",
       "          -3.9927e-02,  1.0038e+00, -1.1435e+00,  1.8040e+00, -2.9009e-02,\n",
       "          -8.1313e-01,  9.0933e-01, -1.1375e+00,  5.1402e-01, -4.8947e-01,\n",
       "          -8.0550e-02,  9.1511e-01, -5.4810e-01,  1.1071e+00, -3.5050e-01,\n",
       "           6.6735e-01, -8.9356e-02,  2.7234e-01,  6.0345e-01,  2.3188e-01,\n",
       "           1.5473e+00, -6.8860e-01, -4.4137e-01,  1.2790e+00, -9.9592e-01,\n",
       "          -4.3626e-01, -8.7003e-01, -5.3829e-02,  1.1496e+00,  1.0411e+00,\n",
       "           5.8017e-02, -1.6868e+00,  4.0054e-01,  1.0880e+00, -4.8284e-01,\n",
       "          -7.0947e-02,  1.0966e+00, -5.6861e-01,  9.0792e-01, -1.7011e-01]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7d9238bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.7691, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)ie(4,8,65)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "\n",
    "        targets = targets.view(B * T)  # (32)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "# idx or xb =(4,8)\n",
    "# returned logits= (4*8,65) stretched vec\n",
    "# losscalculation\n",
    "# The 65-logit vector represents a distribution over choices.\n",
    "# The target integer selects the correct choice, and the loss measures how much probability the model assigned to that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4ba226e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
      "        ...,\n",
      "        [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "        [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3d91c360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7f12db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43, 39, 49,  1, 39, 45, 39, 47, 56, 57, 11,  1, 61, 46, 53,  6,  1, 46,\n",
      "        39, 58, 46,  1, 58, 56,  1, 47, 57,  1, 57, 53,  1, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T = 4, 8\n",
    "print(yb.view(B * T))\n",
    "yb.view(B * T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629579b",
   "metadata": {},
   "source": [
    "loss calculation complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f645a2a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff1d02",
   "metadata": {},
   "source": [
    "# Generate text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce44b09",
   "metadata": {},
   "source": [
    "1. Part 1 dimension calculation for single step of generation and explanation\n",
    "2. Part 2 implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bd618",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9f5d8",
   "metadata": {},
   "source": [
    "for a single tensor sent to predict next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "12573417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.7691, grad_fn=<NllLossBackward0>)\n",
      "idx begin------\n",
      "idx= tensor([[0]])\n",
      "idxshape torch.Size([1, 1])\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 1, 65])\n",
      "logits_prev= tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
      "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
      "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
      "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
      "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
      "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
      "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
      "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
      "          -0.4032, -0.8345]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
      "          0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
      "          1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
      "          0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n",
      "         -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
      "         -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
      "          1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "          1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n",
      "         -0.8345]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0091, 0.0071, 0.0053, 0.0030, 0.0141, 0.0078, 0.0197, 0.0081, 0.0109,\n",
      "         0.0243, 0.0020, 0.0045, 0.0096, 0.0060, 0.0030, 0.0354, 0.0292, 0.0066,\n",
      "         0.0101, 0.0199, 0.0010, 0.0124, 0.0335, 0.0137, 0.0086, 0.0016, 0.0024,\n",
      "         0.0054, 0.0118, 0.0034, 0.0347, 0.0930, 0.0039, 0.0059, 0.0208, 0.0085,\n",
      "         0.0089, 0.0235, 0.0024, 0.0056, 0.0046, 0.0030, 0.0131, 0.0017, 0.0023,\n",
      "         0.0134, 0.0042, 0.0038, 0.0392, 0.0034, 0.0292, 0.0057, 0.0017, 0.0621,\n",
      "         0.1199, 0.0013, 0.0323, 0.0017, 0.0172, 0.0061, 0.0165, 0.0351, 0.0379,\n",
      "         0.0051, 0.0033]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[31]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "ret_idx= [0, 31]\n",
      "len= 2\n",
      "generated_text \n",
      "S\n"
     ]
    }
   ],
   "source": [
    "# for single next token calculation ,max_new_tokens=1\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context(1,1)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #   get new prediction\n",
    "            logits, loss = self(idx)\n",
    "            # returns(batch, time, embedding_dim) ie(B,T,C)->(1,1)->(1,1,65)\n",
    "            # during iteration when idx increases egidx=[31,32] logits, loss = self(idx) returns (1,2,65)\n",
    "            # then logits = logits[:, -1, :]  selects last element of timedim so results(1,65)batch,vocab/contextdim\n",
    "            print(\"---\\n\")\n",
    "            print(\"logit_shape_prev\", logits.shape)\n",
    "            print(\"logits_prev=\", logits)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[\n",
    "                :, -1, :\n",
    "            ]  # becomes (B,C) <-last element in the time dim,,,just one time dim so selects that whole tensor(1,1)->(1,1,65)->(1,65)\n",
    "            # applying softmax to get probabilities form logits\n",
    "            print(\"---\\n\")\n",
    "            print(\"logits_next=\", logits)\n",
    "            print(\"logit_shape_next\", logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            print(\"---\\n\")\n",
    "            print(\"probs=\", probs)\n",
    "            print(\"probsshape\", probs.shape)  # (1,65)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # (B,1)ie(1,1)selects any one token from the probability values from 65 of them\n",
    "            #   append sampled index to the running sequence\n",
    "            # Selects the next token based on the probability of each token, so higher-probability tokens are more likely but not guaranteed.\n",
    "            print(\"---\\n\")\n",
    "            print(\"idx_next=\", idx_next)\n",
    "            print(\"idx_nextshape\", idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            # eg idx=[31,32]\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# PyTorch expects a batch dimension in tensors, so even a single sequence must be shaped as (B, T) rather than just (T).\n",
    "print(\"idx begin------\")\n",
    "print(\"idx=\", idx)\n",
    "print(\"idxshape\", idx.shape)\n",
    "ret_idx = m.generate(idx, max_new_tokens=1)[0].tolist()\n",
    "print(\"ret_idx=\", ret_idx)\n",
    "print(\"len=\", len(ret_idx))\n",
    "print(\"generated_text\", decode_txt(ret_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b5c18d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generatedtext must be \\nS. char 1 by one \\n is treated ans new line here in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2e21f867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 0), (' ', 1), ('!', 2), ('$', 3), ('&', 4)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(strtoint.items())[:5]  # lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6817a2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bf7ab1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([torch.zeros((1), dtype=torch.long).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7fc6bedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_txt([31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "473c9b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.7691, grad_fn=<NllLossBackward0>)\n",
      "idx= tensor([[31]])\n",
      "idxshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  1\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 1, 65])\n",
      "logits_prev= tensor([[[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
      "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
      "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
      "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
      "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
      "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
      "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
      "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "           0.2930,  0.0943]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104, -1.0886,\n",
      "          1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,  1.6030, -0.0726,\n",
      "         -0.4725, -1.1616,  0.5962,  1.3058, -0.7422, -1.2529,  0.6750,  1.5664,\n",
      "         -0.9238, -0.0956, -1.5452, -0.1801,  3.1838, -0.1277,  0.0910,  0.5422,\n",
      "         -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,\n",
      "          1.2588, -0.0644, -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,\n",
      "          1.5294, -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "          0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,\n",
      "          0.0943]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0027, 0.0042, 0.0052, 0.0105, 0.0153, 0.0028, 0.0028, 0.0026, 0.0292,\n",
      "         0.0140, 0.0026, 0.0021, 0.0034, 0.0204, 0.0384, 0.0072, 0.0048, 0.0024,\n",
      "         0.0140, 0.0285, 0.0037, 0.0022, 0.0152, 0.0370, 0.0031, 0.0070, 0.0016,\n",
      "         0.0065, 0.1867, 0.0068, 0.0085, 0.0133, 0.0042, 0.0130, 0.0655, 0.0019,\n",
      "         0.0033, 0.0213, 0.0148, 0.0099, 0.0272, 0.0073, 0.0029, 0.0047, 0.0060,\n",
      "         0.0051, 0.0036, 0.0231, 0.0357, 0.0022, 0.0222, 0.0129, 0.0114, 0.0032,\n",
      "         0.0297, 0.0818, 0.0086, 0.0022, 0.0171, 0.0036, 0.0067, 0.0042, 0.0212,\n",
      "         0.0104, 0.0085]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[38]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  2\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 2, 65])\n",
      "logits_prev= tensor([[[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
      "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
      "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
      "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
      "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
      "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
      "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
      "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "           0.2930,  0.0943],\n",
      "         [-1.6774,  0.7494,  1.4538, -2.2956, -1.6489, -0.9055, -0.4162,\n",
      "          -0.0636, -0.6666,  0.3252, -0.1097,  1.2385,  0.5742,  1.4636,\n",
      "           1.1599, -1.5200, -1.3161, -0.9729, -1.0606,  0.6451, -1.9633,\n",
      "          -0.0105,  0.8958,  0.6347, -0.2572,  0.0489, -0.3030,  0.9117,\n",
      "           0.2386, -1.5984,  1.4978,  0.3207,  0.4126, -0.5031,  0.8067,\n",
      "          -0.2202,  0.8364,  1.3637, -0.6479,  0.8207,  0.4370,  0.0943,\n",
      "           0.8609, -0.2474, -2.3996,  0.3896, -0.4067, -0.9901, -0.7057,\n",
      "          -1.5055, -0.6710, -0.9790, -0.9228, -0.5879,  0.3140, -0.3093,\n",
      "           0.0045, -0.8232, -0.4622, -1.3261,  1.1315, -0.5476,  1.1353,\n",
      "           0.7542, -1.1444]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[-1.6774,  0.7494,  1.4538, -2.2956, -1.6489, -0.9055, -0.4162, -0.0636,\n",
      "         -0.6666,  0.3252, -0.1097,  1.2385,  0.5742,  1.4636,  1.1599, -1.5200,\n",
      "         -1.3161, -0.9729, -1.0606,  0.6451, -1.9633, -0.0105,  0.8958,  0.6347,\n",
      "         -0.2572,  0.0489, -0.3030,  0.9117,  0.2386, -1.5984,  1.4978,  0.3207,\n",
      "          0.4126, -0.5031,  0.8067, -0.2202,  0.8364,  1.3637, -0.6479,  0.8207,\n",
      "          0.4370,  0.0943,  0.8609, -0.2474, -2.3996,  0.3896, -0.4067, -0.9901,\n",
      "         -0.7057, -1.5055, -0.6710, -0.9790, -0.9228, -0.5879,  0.3140, -0.3093,\n",
      "          0.0045, -0.8232, -0.4622, -1.3261,  1.1315, -0.5476,  1.1353,  0.7542,\n",
      "         -1.1444]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0022, 0.0251, 0.0508, 0.0012, 0.0023, 0.0048, 0.0078, 0.0111, 0.0061,\n",
      "         0.0164, 0.0106, 0.0410, 0.0211, 0.0513, 0.0379, 0.0026, 0.0032, 0.0045,\n",
      "         0.0041, 0.0226, 0.0017, 0.0118, 0.0291, 0.0224, 0.0092, 0.0125, 0.0088,\n",
      "         0.0296, 0.0151, 0.0024, 0.0531, 0.0164, 0.0180, 0.0072, 0.0266, 0.0095,\n",
      "         0.0274, 0.0465, 0.0062, 0.0270, 0.0184, 0.0131, 0.0281, 0.0093, 0.0011,\n",
      "         0.0175, 0.0079, 0.0044, 0.0059, 0.0026, 0.0061, 0.0045, 0.0047, 0.0066,\n",
      "         0.0163, 0.0087, 0.0119, 0.0052, 0.0075, 0.0032, 0.0368, 0.0069, 0.0370,\n",
      "         0.0253, 0.0038]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[2]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "************************************************ \n",
      "token no=  3\n",
      "---\n",
      "\n",
      "logit_shape_prev torch.Size([1, 3, 65])\n",
      "logits_prev= tensor([[[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
      "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
      "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
      "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
      "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
      "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
      "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
      "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
      "           0.2930,  0.0943],\n",
      "         [-1.6774,  0.7494,  1.4538, -2.2956, -1.6489, -0.9055, -0.4162,\n",
      "          -0.0636, -0.6666,  0.3252, -0.1097,  1.2385,  0.5742,  1.4636,\n",
      "           1.1599, -1.5200, -1.3161, -0.9729, -1.0606,  0.6451, -1.9633,\n",
      "          -0.0105,  0.8958,  0.6347, -0.2572,  0.0489, -0.3030,  0.9117,\n",
      "           0.2386, -1.5984,  1.4978,  0.3207,  0.4126, -0.5031,  0.8067,\n",
      "          -0.2202,  0.8364,  1.3637, -0.6479,  0.8207,  0.4370,  0.0943,\n",
      "           0.8609, -0.2474, -2.3996,  0.3896, -0.4067, -0.9901, -0.7057,\n",
      "          -1.5055, -0.6710, -0.9790, -0.9228, -0.5879,  0.3140, -0.3093,\n",
      "           0.0045, -0.8232, -0.4622, -1.3261,  1.1315, -0.5476,  1.1353,\n",
      "           0.7542, -1.1444],\n",
      "         [ 1.3035, -0.4501,  1.3471,  1.6910, -0.1244, -1.6824, -0.0266,\n",
      "           0.0740,  1.0517,  0.6779,  0.3067, -0.7472,  0.7435,  0.8877,\n",
      "           2.2874,  0.9611, -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,\n",
      "           1.1385, -0.9913,  0.1700,  1.2249, -0.2345, -1.0572, -0.6543,\n",
      "           1.5909, -0.6995, -0.8961,  0.0662, -0.0563,  2.3412, -2.7234,\n",
      "           0.5097, -0.8145, -0.2460,  0.0045,  2.0474, -0.1575, -0.2187,\n",
      "          -1.3519, -0.0573, -1.8540, -1.3849, -0.3454, -1.1625,  0.1445,\n",
      "           0.1663,  0.7507,  0.9132, -1.7277,  1.3055,  0.9593,  1.0600,\n",
      "           0.6299, -1.2867, -0.6875,  2.1382,  0.5114,  1.2191,  0.1910,\n",
      "          -0.3425,  1.7955]]], grad_fn=<EmbeddingBackward0>)\n",
      "---\n",
      "\n",
      "logits_next= tensor([[ 1.3035, -0.4501,  1.3471,  1.6910, -0.1244, -1.6824, -0.0266,  0.0740,\n",
      "          1.0517,  0.6779,  0.3067, -0.7472,  0.7435,  0.8877,  2.2874,  0.9611,\n",
      "         -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,  1.1385, -0.9913,  0.1700,\n",
      "          1.2249, -0.2345, -1.0572, -0.6543,  1.5909, -0.6995, -0.8961,  0.0662,\n",
      "         -0.0563,  2.3412, -2.7234,  0.5097, -0.8145, -0.2460,  0.0045,  2.0474,\n",
      "         -0.1575, -0.2187, -1.3519, -0.0573, -1.8540, -1.3849, -0.3454, -1.1625,\n",
      "          0.1445,  0.1663,  0.7507,  0.9132, -1.7277,  1.3055,  0.9593,  1.0600,\n",
      "          0.6299, -1.2867, -0.6875,  2.1382,  0.5114,  1.2191,  0.1910, -0.3425,\n",
      "          1.7955]], grad_fn=<SelectBackward0>)\n",
      "logit_shape_next torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "probs= tensor([[0.0288, 0.0050, 0.0301, 0.0424, 0.0069, 0.0015, 0.0076, 0.0084, 0.0224,\n",
      "         0.0154, 0.0106, 0.0037, 0.0164, 0.0190, 0.0770, 0.0204, 0.0017, 0.0058,\n",
      "         0.0070, 0.0057, 0.0042, 0.0244, 0.0029, 0.0093, 0.0266, 0.0062, 0.0027,\n",
      "         0.0041, 0.0384, 0.0039, 0.0032, 0.0084, 0.0074, 0.0813, 0.0005, 0.0130,\n",
      "         0.0035, 0.0061, 0.0079, 0.0606, 0.0067, 0.0063, 0.0020, 0.0074, 0.0012,\n",
      "         0.0020, 0.0055, 0.0024, 0.0090, 0.0092, 0.0166, 0.0195, 0.0014, 0.0288,\n",
      "         0.0204, 0.0226, 0.0147, 0.0022, 0.0039, 0.0663, 0.0130, 0.0265, 0.0095,\n",
      "         0.0056, 0.0471]], grad_fn=<SoftmaxBackward0>)\n",
      "probsshape torch.Size([1, 65])\n",
      "---\n",
      "\n",
      "idx_next= tensor([[21]])\n",
      "idx_nextshape torch.Size([1, 1])\n",
      "ret_idx= [31, 38, 2, 21]\n",
      "len= 4\n",
      "generated_text SZ!I\n"
     ]
    }
   ],
   "source": [
    "# for max_new_tokens=3\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            print(\"******\" * 8, \"\\ntoken no= \", _ + 1)\n",
    "            #   get new predication\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            print(\"---\\n\")\n",
    "            print(\"logit_shape_prev\", logits.shape)\n",
    "            print(\"logits_prev=\", logits)\n",
    "            logits = logits[:, -1, :]  # becomes (B,C) <-last element in the time dim\n",
    "            # applying softmax to get probabilities form logits\n",
    "            print(\"---\\n\")\n",
    "            print(\"logits_next=\", logits)\n",
    "            print(\"logit_shape_next\", logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            print(\"---\\n\")\n",
    "            print(\"probs=\", probs)\n",
    "            print(\"probsshape\", probs.shape)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "            #   append sampled index to the running sequence\n",
    "            print(\"---\\n\")\n",
    "            print(\"idx_next=\", idx_next)\n",
    "            print(\"idx_nextshape\", idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.tensor([[31]], dtype=torch.long)\n",
    "print(\"idx=\", idx)\n",
    "print(\"idxshape\", idx.shape)\n",
    "ret_idx = m.generate((idx), max_new_tokens=3)[0].tolist()\n",
    "print(\"ret_idx=\", ret_idx)\n",
    "print(\"len=\", len(ret_idx))\n",
    "print(\"generated_text\", decode_txt(ret_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7108a5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[31]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9096abad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97df0b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b3a24",
   "metadata": {},
   "source": [
    "# Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb34161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits torch.Size([32, 65]) \n",
      " loss=  tensor(4.7691, grad_fn=<NllLossBackward0>)\n",
      "idx begin------\n",
      "idx= tensor([[0]])\n",
      "idxshape torch.Size([1, 1])\n",
      "ret_idx= [0, 31, 23, 21, 41, 24, 32, 11, 13, 41, 17, 24, 25, 53, 32, 40, 60, 38, 60, 1, 15, 12, 52, 55, 7, 29, 17, 9, 9, 10, 15, 22, 55, 49, 27, 23, 20, 7, 55, 11, 10, 50, 39, 2, 53, 47, 63, 61, 49, 20, 48, 45, 15, 46, 64, 40, 29, 12, 59, 2, 9, 40, 24, 21, 45, 61, 43, 60, 51, 63, 18, 22, 19, 33, 19, 54, 0, 61, 52, 37, 35, 51, 52, 62, 23, 35, 35, 43, 60, 7, 58, 16, 55, 36, 17, 56, 34, 23, 24, 45, 22]\n",
      "len= 101\n",
      "----\n",
      " generated_text ->  \n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# max_new_tokens=100\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # 65*65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and target are both (B,T)tensor of integer B-batch ,T-time/block_size/context length, C-channel. (here b=4,T=8,C=vocabsize ie 65)\n",
    "\n",
    "        logits = self.token_embedding_table(idx)  # logits becomes (B,T,C)ie(4,8,65)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (32*65) stretching the vec\n",
    "            targets = targets.view(B * T)  # (32)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # takes (B,T) and generate work is to generate (b,T+1,T+2)ie generate new token in time dim ie(contextlength dim)\n",
    "        # idx is (B,T) array of indices in the current context(1,1)\n",
    "        for _ in range(max_new_tokens):\n",
    "            #   get new prediction\n",
    "            logits, loss = self(idx)\n",
    "            # returns(batch, time, embedding_dim) ie(B,T,C)->(1,1)->(1,1,65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[\n",
    "                :, -1, :\n",
    "            ]  # becomes (B,C) <-last element in the time dim,,,just one time dim so selects that whole tensor(1,1)->(1,1,65)->(1,65)\n",
    "            # applying softmax to get probabilities form logits\n",
    "            probs = F.softmax(logits, dim=-1)  # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(\n",
    "                probs, num_samples=1\n",
    "            )  # (B,1)ie(1,1)selects any one token from the probability values from 65 of them\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B,T+1)\n",
    "            # eg next = idx=[31,32]\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(\"logits\", logits.shape, \"\\n loss= \", loss)\n",
    "\n",
    "\n",
    "# --------\n",
    "# generate\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "# 0 index in vocab represents \\n\n",
    "# PyTorch expects a batch dimension in tensors, so even a single sequence must be shaped as (B, T) rather than just (T).\n",
    "print(\"idx begin------\")\n",
    "print(\"idx=\", idx)\n",
    "print(\"idxshape\", idx.shape)\n",
    "ret_idx = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "print(\"ret_idx=\", ret_idx)\n",
    "print(\"len=\", len(ret_idx))\n",
    "print(\"----\\n generated_text -> \", decode_txt(ret_idx))\n",
    "# print(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8236bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    decode_txt(\n",
    "        m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=600)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52eb39",
   "metadata": {},
   "source": [
    "result is sort of garbage because next token is predicted only on the basis of current token without any context of previous tokens.\n",
    "for prediction of T SKIcLT model only sees L and predicts next token T ,without any context of previous tokens SKIc.\n",
    "\n",
    "- next task is to make model see previous tokens as context not just the current token while predicting next token.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "38f4d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets improve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ef85b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "batch_size = 32  # previously 4\n",
    "lossitm = []\n",
    "for steps in range(20000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lossitm.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "520b75c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4557886123657227,\n",
       " 2.3607707023620605,\n",
       " 2.4638681411743164,\n",
       " 2.3458073139190674,\n",
       " 2.456632375717163,\n",
       " 2.352128028869629,\n",
       " 2.5676419734954834,\n",
       " 2.427316427230835,\n",
       " 2.47452449798584,\n",
       " 2.5497617721557617]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossitm[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "def10979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x162ec4210>]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossitm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUpld thoof n lot. nde.\n",
      "\n",
      "Lou onduss femyour ecealithinsthif twifre:\n",
      "\n",
      "JUCHis s, my wecredous mpl; d an hen h Whechy ls ait osibust h, th ce pre aloods MABread ite.\n",
      "KESThe t\n",
      "ws.\n",
      "thug t whr, wod,\n",
      "I iso\n",
      "Sibe BRDougit owe wampous ourewe u dous o nd'ly, i'sourlofilay, tat mefootope k hthe.\n",
      "Thl woug,\n",
      "\n",
      "To swounat mbat apol gankier; uld he ass clyounereaishin ceyof isous, ol\n",
      "\n",
      "ifense fard, diforataris nnigur peald praw aneweiveathave:\n",
      "I wad? Mat, puld st s s mease a mame it nnco sem. ho elesess bar IO,\n",
      "DUSo?\n",
      "Gour banindo s, hes m'drl, towe ds, th that HED nshe ckind m ne n rithar mburderge dsth br,\n",
      "Hido\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    decode_txt(\n",
    "        m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=600)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slight improvement in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a1e1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc4e2c81",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "Self-attention is a mechanism that allows a model to weigh the importance of different tokens in a sequence when making predictions. In the context of language modeling, self-attention enables the model to consider the relationships between words in a sentence, allowing it to capture context and dependencies more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d6e36",
   "metadata": {},
   "source": [
    "- for the current token to attend to previous tokens, we need to mask the future tokens.\n",
    "- for the current token ,lets average the embeddings of all previous tokens and use that to predict the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b32af8",
   "metadata": {},
   "source": [
    "```\n",
    "B,T,C=4,8,2\n",
    "x=torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "# we want x[b,t]=mean_{i<=t} x[b,i]\n",
    "\n",
    "xbow=torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev=x[b,:t+1] #t,c\n",
    "        print(xprev)\n",
    "        xbow[b,t]=torch.mean(xprev,0)\n",
    "```\n",
    "\n",
    "xbow represents the averaged embeddings of all previous tokens for each token in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6026fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eg\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b39131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.2858,  0.9651],\n",
       "         [-2.0371,  0.4931],\n",
       "         [ 1.4870,  0.5910],\n",
       "         [ 0.1260, -1.5627],\n",
       "         [-1.1601, -0.3348],\n",
       "         [ 0.4478, -0.8016],\n",
       "         [ 1.5236,  2.5086]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 1.0101,  0.1215],\n",
       "         [ 0.1584,  1.1340],\n",
       "         [-1.1539, -0.2984],\n",
       "         [-0.5075, -0.9239],\n",
       "         [ 0.5467, -1.4948],\n",
       "         [-1.2057,  0.5718],\n",
       "         [-0.5974, -0.6937]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.3514, -0.2759],\n",
       "         [-1.5108,  2.1048],\n",
       "         [ 2.7630, -1.7465],\n",
       "         [ 1.4516, -1.5103],\n",
       "         [ 0.8212, -0.2115],\n",
       "         [ 0.7789,  1.5333],\n",
       "         [ 1.6097, -0.4032]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8345,  0.5978]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646],\n",
      "        [-0.4970,  0.4658]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646],\n",
      "        [-0.4970,  0.4658],\n",
      "        [-0.2573, -1.0673]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646],\n",
      "        [-0.4970,  0.4658],\n",
      "        [-0.2573, -1.0673],\n",
      "        [ 2.0089, -0.5370]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646],\n",
      "        [-0.4970,  0.4658],\n",
      "        [-0.2573, -1.0673],\n",
      "        [ 2.0089, -0.5370],\n",
      "        [ 0.2228,  0.6971]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646],\n",
      "        [-0.4970,  0.4658],\n",
      "        [-0.2573, -1.0673],\n",
      "        [ 2.0089, -0.5370],\n",
      "        [ 0.2228,  0.6971],\n",
      "        [-1.4267,  0.9059]])\n",
      "tensor([[-0.8345,  0.5978],\n",
      "        [-0.0514, -0.0646],\n",
      "        [-0.4970,  0.4658],\n",
      "        [-0.2573, -1.0673],\n",
      "        [ 2.0089, -0.5370],\n",
      "        [ 0.2228,  0.6971],\n",
      "        [-1.4267,  0.9059],\n",
      "        [ 0.1446,  0.2280]])\n",
      "tensor([[ 2.4900, -1.2237]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560],\n",
      "        [-1.5935, -1.2706]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560],\n",
      "        [-1.5935, -1.2706],\n",
      "        [ 0.6903, -0.1961]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560],\n",
      "        [-1.5935, -1.2706],\n",
      "        [ 0.6903, -0.1961],\n",
      "        [ 0.3449, -0.3419]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560],\n",
      "        [-1.5935, -1.2706],\n",
      "        [ 0.6903, -0.1961],\n",
      "        [ 0.3449, -0.3419],\n",
      "        [ 0.4759, -0.7663]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560],\n",
      "        [-1.5935, -1.2706],\n",
      "        [ 0.6903, -0.1961],\n",
      "        [ 0.3449, -0.3419],\n",
      "        [ 0.4759, -0.7663],\n",
      "        [-0.4190, -0.4370]])\n",
      "tensor([[ 2.4900, -1.2237],\n",
      "        [ 1.0107,  0.5560],\n",
      "        [-1.5935, -1.2706],\n",
      "        [ 0.6903, -0.1961],\n",
      "        [ 0.3449, -0.3419],\n",
      "        [ 0.4759, -0.7663],\n",
      "        [-0.4190, -0.4370],\n",
      "        [-1.0012, -0.4094]])\n",
      "tensor([[-1.6669, -1.3651]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419],\n",
      "        [-0.2978,  0.0172]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419],\n",
      "        [-0.2978,  0.0172],\n",
      "        [-0.1772, -0.1334]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419],\n",
      "        [-0.2978,  0.0172],\n",
      "        [-0.1772, -0.1334],\n",
      "        [ 0.2940,  1.3850]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419],\n",
      "        [-0.2978,  0.0172],\n",
      "        [-0.1772, -0.1334],\n",
      "        [ 0.2940,  1.3850],\n",
      "        [ 0.1209,  2.5418]])\n",
      "tensor([[-1.6669, -1.3651],\n",
      "        [-0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419],\n",
      "        [-0.2978,  0.0172],\n",
      "        [-0.1772, -0.1334],\n",
      "        [ 0.2940,  1.3850],\n",
      "        [ 0.1209,  2.5418],\n",
      "        [-0.6405, -1.9740]])\n",
      "tensor([[-0.3296,  0.0080]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846],\n",
      "        [ 0.1670,  0.4586]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846],\n",
      "        [ 0.1670,  0.4586],\n",
      "        [-1.7662,  0.5860]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846],\n",
      "        [ 0.1670,  0.4586],\n",
      "        [-1.7662,  0.5860],\n",
      "        [ 1.7510,  0.2807]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846],\n",
      "        [ 0.1670,  0.4586],\n",
      "        [-1.7662,  0.5860],\n",
      "        [ 1.7510,  0.2807],\n",
      "        [ 0.3110, -0.6538]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846],\n",
      "        [ 0.1670,  0.4586],\n",
      "        [-1.7662,  0.5860],\n",
      "        [ 1.7510,  0.2807],\n",
      "        [ 0.3110, -0.6538],\n",
      "        [-0.6576,  0.3184]])\n",
      "tensor([[-0.3296,  0.0080],\n",
      "        [ 0.9262, -1.8846],\n",
      "        [ 0.1670,  0.4586],\n",
      "        [-1.7662,  0.5860],\n",
      "        [ 1.7510,  0.2807],\n",
      "        [ 0.3110, -0.6538],\n",
      "        [-0.6576,  0.3184],\n",
      "        [-0.5496, -1.4649]])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "# we want x[b,t]=mean_{i<=t} x[b,i]\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]  # t,c\n",
    "        print(xprev)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2c3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.0894)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array([0.1808, -0.3596]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1130f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8345,  0.5978],\n",
       "        [-0.0514, -0.0646],\n",
       "        [-0.4970,  0.4658],\n",
       "        [-0.2573, -1.0673],\n",
       "        [ 2.0089, -0.5370],\n",
       "        [ 0.2228,  0.6971],\n",
       "        [-1.4267,  0.9059],\n",
       "        [ 0.1446,  0.2280]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af56bddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8345,  0.5978],\n",
       "        [-0.4429,  0.2666],\n",
       "        [-0.4610,  0.3330],\n",
       "        [-0.4100, -0.0171],\n",
       "        [ 0.0738, -0.1210],\n",
       "        [ 0.0986,  0.0153],\n",
       "        [-0.1193,  0.1425],\n",
       "        [-0.0863,  0.1532]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65151c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 2]), torch.Size([4, 8, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      " a=  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      " b=  tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      " c=  tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"--\\n a= \", a)\n",
    "print(\"--\\n b= \", b)\n",
    "print(\"--\\n c= \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241b2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2af89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50510f3f",
   "metadata": {},
   "source": [
    "# masking future tokens\n",
    "\n",
    "torch.tril(torch.ones(3, 3))\n",
    "\n",
    "```\n",
    "tensor([[1., 0., 0.],\n",
    "        [1., 1., 0.],\n",
    "        [1., 1., 1.]])\n",
    "```\n",
    "\n",
    "- The lower triangular matrix created using `torch.tril` serves as a mask to prevent the model from seeing to future tokens during self-attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      " a=  tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      " b=  tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      " c=  tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3, 3)))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"--\\n a= \", a)\n",
    "print(\"--\\n b= \", b)\n",
    "print(\"--\\n c= \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28793dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      " a=  tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      " b=  tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      " c=  tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3, 3)))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"--\\n a= \", a)\n",
    "print(\"--\\n b= \", b)\n",
    "print(\"--\\n c= \", c)\n",
    "\n",
    "# each row sum to one and  c results to the average of each rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eaa9039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcda0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same operation different  ways\n",
    "#\n",
    "# ------- version 1\n",
    "# B,T,C=4,8,2\n",
    "# x=torch.randn(B,T,C)\n",
    "# x.shape\n",
    "\n",
    "# # we want x[b,t]=mean_{i<=t} x[b,i]\n",
    "\n",
    "# xbow=torch.zeros((B,T,C))\n",
    "# for b in range(B):\n",
    "#     for t in range(T):\n",
    "#         xprev=x[b,:t+1] #t,c\n",
    "#         print(xprev)\n",
    "#         xbow[b,t]=torch.mean(xprev,0)\n",
    "\n",
    "# ----- version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x  # (T,T)*(B,T,C) so it does (B,T,T)*(B,T,C) and returns-> (B,T,C)\n",
    "\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d9f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8345,  0.5978],\n",
       "         [-0.4429,  0.2666],\n",
       "         [-0.4610,  0.3330],\n",
       "         [-0.4100, -0.0171],\n",
       "         [ 0.0738, -0.1210],\n",
       "         [ 0.0986,  0.0153],\n",
       "         [-0.1193,  0.1425],\n",
       "         [-0.0863,  0.1532]]),\n",
       " tensor([[-0.8345,  0.5978],\n",
       "         [-0.4429,  0.2666],\n",
       "         [-0.4610,  0.3330],\n",
       "         [-0.4100, -0.0171],\n",
       "         [ 0.0738, -0.1210],\n",
       "         [ 0.0986,  0.0153],\n",
       "         [-0.1193,  0.1425],\n",
       "         [-0.0863,  0.1532]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2071924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "# T=8\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triangular matrix\n",
    "wei = torch.zeros((T, T))  # attention weights\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))  # mask future tokens\n",
    "wei = F.softmax(wei, dim=1)  # softmax to get probabilities\n",
    "\n",
    "xbow3 = wei @ x  # matrix multiplication to get weighted average of embeddings\n",
    "\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f511225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8345,  0.5978],\n",
       "         [-0.4429,  0.2666],\n",
       "         [-0.4610,  0.3330],\n",
       "         [-0.4100, -0.0171],\n",
       "         [ 0.0738, -0.1210],\n",
       "         [ 0.0986,  0.0153],\n",
       "         [-0.1193,  0.1425],\n",
       "         [-0.0863,  0.1532]]),\n",
       " tensor([[-0.8345,  0.5978],\n",
       "         [-0.4429,  0.2666],\n",
       "         [-0.4610,  0.3330],\n",
       "         [-0.4100, -0.0171],\n",
       "         [ 0.0738, -0.1210],\n",
       "         [ 0.0986,  0.0153],\n",
       "         [-0.1193,  0.1425],\n",
       "         [-0.0863,  0.1532]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf9385",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same operation different  ways\n",
    "#\n",
    "# ------- version 1\n",
    "# B,T,C=4,8,2\n",
    "# x=torch.randn(B,T,C)\n",
    "# x.shape\n",
    "\n",
    "# # we want x[b,t]=mean_{i<=t} x[b,i]\n",
    "\n",
    "# xbow=torch.zeros((B,T,C))\n",
    "# for b in range(B):\n",
    "#     for t in range(T):\n",
    "#         xprev=x[b,:t+1] #t,c\n",
    "#         print(xprev)\n",
    "#         xbow[b,t]=torch.mean(xprev,0)\n",
    "\n",
    "# ----- version 2\n",
    "# wei = torch.tril(torch.ones(T, T))\n",
    "# wei = wei / wei.sum(1, keepdim=True)\n",
    "# xbow2 = wei @ x  # (T,T)*(B,T,C) so it does (B,T,T)*(B,T,C) and returns-> (B,T,C)\n",
    "\n",
    "# torch.allclose(xbow,xbow2)\n",
    "\n",
    "# ----- version 3\n",
    "# T=8\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triangular matrix\n",
    "wei = torch.zeros((T, T))  # attention weights\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))  # mask future tokens\n",
    "wei = F.softmax(wei, dim=1)  # softmax to get probabilities\n",
    "\n",
    "xbow3 = wei @ x  # matrix multiplication to get weighted average of embeddings\n",
    "\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07905451",
   "metadata": {},
   "source": [
    "-  we can perform weighted aggregateion by matrix multiplication of past tokens by using lower triangular matrix as mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded97e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_ds_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
